{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahlet/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(100,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahlet/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:975: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 1.039234, acc.: 50.00%] [G loss: 0.790982]\n",
      "1 [D loss: 0.447138, acc.: 78.12%] [G loss: 0.819692]\n",
      "2 [D loss: 0.352639, acc.: 84.38%] [G loss: 0.895983]\n",
      "3 [D loss: 0.288668, acc.: 89.06%] [G loss: 1.006849]\n",
      "4 [D loss: 0.259122, acc.: 93.75%] [G loss: 1.144986]\n",
      "5 [D loss: 0.227110, acc.: 96.88%] [G loss: 1.248038]\n",
      "6 [D loss: 0.185942, acc.: 96.88%] [G loss: 1.342166]\n",
      "7 [D loss: 0.161515, acc.: 100.00%] [G loss: 1.426543]\n",
      "8 [D loss: 0.164785, acc.: 96.88%] [G loss: 1.501548]\n",
      "9 [D loss: 0.157062, acc.: 100.00%] [G loss: 1.667943]\n",
      "10 [D loss: 0.119932, acc.: 100.00%] [G loss: 1.643316]\n",
      "11 [D loss: 0.122005, acc.: 100.00%] [G loss: 1.800172]\n",
      "12 [D loss: 0.119157, acc.: 98.44%] [G loss: 1.827316]\n",
      "13 [D loss: 0.101439, acc.: 100.00%] [G loss: 2.033730]\n",
      "14 [D loss: 0.098787, acc.: 100.00%] [G loss: 1.959269]\n",
      "15 [D loss: 0.086246, acc.: 100.00%] [G loss: 2.032908]\n",
      "16 [D loss: 0.079014, acc.: 100.00%] [G loss: 2.088560]\n",
      "17 [D loss: 0.076932, acc.: 100.00%] [G loss: 2.216519]\n",
      "18 [D loss: 0.079956, acc.: 100.00%] [G loss: 2.241627]\n",
      "19 [D loss: 0.062619, acc.: 100.00%] [G loss: 2.383846]\n",
      "20 [D loss: 0.067816, acc.: 100.00%] [G loss: 2.393980]\n",
      "21 [D loss: 0.074227, acc.: 100.00%] [G loss: 2.435448]\n",
      "22 [D loss: 0.048106, acc.: 100.00%] [G loss: 2.494817]\n",
      "23 [D loss: 0.055319, acc.: 100.00%] [G loss: 2.601654]\n",
      "24 [D loss: 0.056483, acc.: 100.00%] [G loss: 2.706868]\n",
      "25 [D loss: 0.057328, acc.: 100.00%] [G loss: 2.812000]\n",
      "26 [D loss: 0.042800, acc.: 100.00%] [G loss: 2.815788]\n",
      "27 [D loss: 0.040863, acc.: 100.00%] [G loss: 2.651661]\n",
      "28 [D loss: 0.037155, acc.: 100.00%] [G loss: 2.864639]\n",
      "29 [D loss: 0.037847, acc.: 100.00%] [G loss: 2.826577]\n",
      "30 [D loss: 0.033886, acc.: 100.00%] [G loss: 2.864329]\n",
      "31 [D loss: 0.035997, acc.: 100.00%] [G loss: 2.977875]\n",
      "32 [D loss: 0.035945, acc.: 100.00%] [G loss: 3.015707]\n",
      "33 [D loss: 0.039453, acc.: 100.00%] [G loss: 3.082320]\n",
      "34 [D loss: 0.037014, acc.: 100.00%] [G loss: 3.088184]\n",
      "35 [D loss: 0.037737, acc.: 100.00%] [G loss: 3.111873]\n",
      "36 [D loss: 0.031407, acc.: 100.00%] [G loss: 3.072402]\n",
      "37 [D loss: 0.035084, acc.: 100.00%] [G loss: 3.172940]\n",
      "38 [D loss: 0.032841, acc.: 100.00%] [G loss: 3.212043]\n",
      "39 [D loss: 0.033896, acc.: 100.00%] [G loss: 3.440636]\n",
      "40 [D loss: 0.026044, acc.: 100.00%] [G loss: 3.188631]\n",
      "41 [D loss: 0.030901, acc.: 100.00%] [G loss: 3.376801]\n",
      "42 [D loss: 0.029246, acc.: 100.00%] [G loss: 3.498727]\n",
      "43 [D loss: 0.019983, acc.: 100.00%] [G loss: 3.554666]\n",
      "44 [D loss: 0.030796, acc.: 100.00%] [G loss: 3.358819]\n",
      "45 [D loss: 0.025043, acc.: 100.00%] [G loss: 3.609014]\n",
      "46 [D loss: 0.021108, acc.: 100.00%] [G loss: 3.539517]\n",
      "47 [D loss: 0.017640, acc.: 100.00%] [G loss: 3.577893]\n",
      "48 [D loss: 0.024432, acc.: 100.00%] [G loss: 3.590679]\n",
      "49 [D loss: 0.023433, acc.: 100.00%] [G loss: 3.536818]\n",
      "50 [D loss: 0.021583, acc.: 100.00%] [G loss: 3.656793]\n",
      "51 [D loss: 0.031299, acc.: 100.00%] [G loss: 3.659193]\n",
      "52 [D loss: 0.023367, acc.: 100.00%] [G loss: 3.772390]\n",
      "53 [D loss: 0.020507, acc.: 100.00%] [G loss: 3.796568]\n",
      "54 [D loss: 0.027018, acc.: 100.00%] [G loss: 3.737694]\n",
      "55 [D loss: 0.022882, acc.: 100.00%] [G loss: 3.696404]\n",
      "56 [D loss: 0.017382, acc.: 100.00%] [G loss: 3.681397]\n",
      "57 [D loss: 0.018476, acc.: 100.00%] [G loss: 3.769855]\n",
      "58 [D loss: 0.021816, acc.: 100.00%] [G loss: 3.734716]\n",
      "59 [D loss: 0.020402, acc.: 100.00%] [G loss: 3.899069]\n",
      "60 [D loss: 0.015459, acc.: 100.00%] [G loss: 3.828815]\n",
      "61 [D loss: 0.016204, acc.: 100.00%] [G loss: 3.897382]\n",
      "62 [D loss: 0.013775, acc.: 100.00%] [G loss: 3.947365]\n",
      "63 [D loss: 0.016277, acc.: 100.00%] [G loss: 3.668252]\n",
      "64 [D loss: 0.021060, acc.: 100.00%] [G loss: 3.676636]\n",
      "65 [D loss: 0.020136, acc.: 100.00%] [G loss: 3.838189]\n",
      "66 [D loss: 0.016038, acc.: 100.00%] [G loss: 3.955921]\n",
      "67 [D loss: 0.024198, acc.: 100.00%] [G loss: 3.961945]\n",
      "68 [D loss: 0.016965, acc.: 100.00%] [G loss: 3.857255]\n",
      "69 [D loss: 0.021921, acc.: 100.00%] [G loss: 3.753980]\n",
      "70 [D loss: 0.024149, acc.: 100.00%] [G loss: 4.040401]\n",
      "71 [D loss: 0.018830, acc.: 100.00%] [G loss: 4.084745]\n",
      "72 [D loss: 0.021361, acc.: 100.00%] [G loss: 4.160828]\n",
      "73 [D loss: 0.024491, acc.: 100.00%] [G loss: 4.230865]\n",
      "74 [D loss: 0.012351, acc.: 100.00%] [G loss: 4.309849]\n",
      "75 [D loss: 0.015761, acc.: 100.00%] [G loss: 4.201413]\n",
      "76 [D loss: 0.015150, acc.: 100.00%] [G loss: 4.132524]\n",
      "77 [D loss: 0.015447, acc.: 100.00%] [G loss: 4.203922]\n",
      "78 [D loss: 0.017203, acc.: 100.00%] [G loss: 4.230277]\n",
      "79 [D loss: 0.017351, acc.: 100.00%] [G loss: 4.233404]\n",
      "80 [D loss: 0.015858, acc.: 100.00%] [G loss: 4.162731]\n",
      "81 [D loss: 0.014534, acc.: 100.00%] [G loss: 4.218799]\n",
      "82 [D loss: 0.018481, acc.: 100.00%] [G loss: 4.196222]\n",
      "83 [D loss: 0.014842, acc.: 100.00%] [G loss: 4.352200]\n",
      "84 [D loss: 0.014779, acc.: 100.00%] [G loss: 4.401386]\n",
      "85 [D loss: 0.014055, acc.: 100.00%] [G loss: 4.299183]\n",
      "86 [D loss: 0.018001, acc.: 100.00%] [G loss: 4.265055]\n",
      "87 [D loss: 0.018332, acc.: 100.00%] [G loss: 4.219393]\n",
      "88 [D loss: 0.021624, acc.: 100.00%] [G loss: 4.292559]\n",
      "89 [D loss: 0.019337, acc.: 100.00%] [G loss: 4.476443]\n",
      "90 [D loss: 0.012545, acc.: 100.00%] [G loss: 4.423710]\n",
      "91 [D loss: 0.015175, acc.: 100.00%] [G loss: 4.482693]\n",
      "92 [D loss: 0.025407, acc.: 100.00%] [G loss: 4.573468]\n",
      "93 [D loss: 0.018066, acc.: 100.00%] [G loss: 4.396919]\n",
      "94 [D loss: 0.019039, acc.: 100.00%] [G loss: 4.667513]\n",
      "95 [D loss: 0.018438, acc.: 100.00%] [G loss: 4.596878]\n",
      "96 [D loss: 0.015634, acc.: 100.00%] [G loss: 4.559026]\n",
      "97 [D loss: 0.025966, acc.: 100.00%] [G loss: 4.830879]\n",
      "98 [D loss: 0.015132, acc.: 100.00%] [G loss: 4.678295]\n",
      "99 [D loss: 0.011858, acc.: 100.00%] [G loss: 4.504962]\n",
      "100 [D loss: 0.018591, acc.: 100.00%] [G loss: 4.600415]\n",
      "101 [D loss: 0.020227, acc.: 100.00%] [G loss: 4.621717]\n",
      "102 [D loss: 0.022895, acc.: 100.00%] [G loss: 4.942530]\n",
      "103 [D loss: 0.038318, acc.: 100.00%] [G loss: 4.777659]\n",
      "104 [D loss: 0.021922, acc.: 100.00%] [G loss: 4.636279]\n",
      "105 [D loss: 0.018876, acc.: 100.00%] [G loss: 4.784523]\n",
      "106 [D loss: 0.013920, acc.: 100.00%] [G loss: 4.864223]\n",
      "107 [D loss: 0.025320, acc.: 100.00%] [G loss: 4.633784]\n",
      "108 [D loss: 0.016049, acc.: 100.00%] [G loss: 4.949461]\n",
      "109 [D loss: 0.036618, acc.: 100.00%] [G loss: 5.151143]\n",
      "110 [D loss: 0.033658, acc.: 100.00%] [G loss: 4.670958]\n",
      "111 [D loss: 0.032316, acc.: 98.44%] [G loss: 5.031861]\n",
      "112 [D loss: 0.047458, acc.: 98.44%] [G loss: 5.181499]\n",
      "113 [D loss: 0.058062, acc.: 98.44%] [G loss: 5.409264]\n",
      "114 [D loss: 0.158827, acc.: 90.62%] [G loss: 5.270299]\n",
      "115 [D loss: 0.047609, acc.: 100.00%] [G loss: 5.540424]\n",
      "116 [D loss: 0.037954, acc.: 100.00%] [G loss: 5.258706]\n",
      "117 [D loss: 0.012244, acc.: 100.00%] [G loss: 5.068452]\n",
      "118 [D loss: 0.026518, acc.: 100.00%] [G loss: 4.903019]\n",
      "119 [D loss: 0.076591, acc.: 95.31%] [G loss: 5.207949]\n",
      "120 [D loss: 0.059771, acc.: 98.44%] [G loss: 5.327395]\n",
      "121 [D loss: 0.094839, acc.: 96.88%] [G loss: 4.621670]\n",
      "122 [D loss: 0.063773, acc.: 96.88%] [G loss: 5.564701]\n",
      "123 [D loss: 0.456950, acc.: 76.56%] [G loss: 5.670844]\n",
      "124 [D loss: 0.062246, acc.: 98.44%] [G loss: 5.455967]\n",
      "125 [D loss: 0.120847, acc.: 95.31%] [G loss: 5.352324]\n",
      "126 [D loss: 0.229379, acc.: 92.19%] [G loss: 5.095585]\n",
      "127 [D loss: 0.070972, acc.: 98.44%] [G loss: 5.795276]\n",
      "128 [D loss: 0.794185, acc.: 68.75%] [G loss: 3.993469]\n",
      "129 [D loss: 0.186597, acc.: 93.75%] [G loss: 4.299156]\n",
      "130 [D loss: 0.165305, acc.: 92.19%] [G loss: 5.309849]\n",
      "131 [D loss: 0.099987, acc.: 100.00%] [G loss: 4.397478]\n",
      "132 [D loss: 0.136523, acc.: 93.75%] [G loss: 4.477837]\n",
      "133 [D loss: 0.094909, acc.: 96.88%] [G loss: 4.583810]\n",
      "134 [D loss: 0.129044, acc.: 93.75%] [G loss: 4.321688]\n",
      "135 [D loss: 0.121842, acc.: 95.31%] [G loss: 4.473348]\n",
      "136 [D loss: 0.156298, acc.: 93.75%] [G loss: 4.291803]\n",
      "137 [D loss: 0.120240, acc.: 98.44%] [G loss: 4.189507]\n",
      "138 [D loss: 0.312907, acc.: 82.81%] [G loss: 4.298590]\n",
      "139 [D loss: 0.236480, acc.: 92.19%] [G loss: 4.062554]\n",
      "140 [D loss: 0.115158, acc.: 98.44%] [G loss: 4.364066]\n",
      "141 [D loss: 0.811733, acc.: 79.69%] [G loss: 3.235125]\n",
      "142 [D loss: 0.102854, acc.: 96.88%] [G loss: 4.453632]\n",
      "143 [D loss: 1.107923, acc.: 59.38%] [G loss: 3.281180]\n",
      "144 [D loss: 0.417116, acc.: 81.25%] [G loss: 2.705558]\n",
      "145 [D loss: 0.243438, acc.: 89.06%] [G loss: 4.291637]\n",
      "146 [D loss: 0.055002, acc.: 100.00%] [G loss: 4.233258]\n",
      "147 [D loss: 0.133747, acc.: 98.44%] [G loss: 4.036923]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148 [D loss: 0.231609, acc.: 87.50%] [G loss: 3.312974]\n",
      "149 [D loss: 0.154273, acc.: 95.31%] [G loss: 3.889272]\n",
      "150 [D loss: 0.349878, acc.: 82.81%] [G loss: 3.270880]\n",
      "151 [D loss: 0.143616, acc.: 96.88%] [G loss: 4.284002]\n",
      "152 [D loss: 0.706647, acc.: 75.00%] [G loss: 2.936396]\n",
      "153 [D loss: 0.094626, acc.: 98.44%] [G loss: 3.507365]\n",
      "154 [D loss: 0.098106, acc.: 98.44%] [G loss: 3.526769]\n",
      "155 [D loss: 0.218619, acc.: 89.06%] [G loss: 3.582727]\n",
      "156 [D loss: 0.452143, acc.: 73.44%] [G loss: 3.973213]\n",
      "157 [D loss: 0.269161, acc.: 89.06%] [G loss: 3.347941]\n",
      "158 [D loss: 0.170253, acc.: 96.88%] [G loss: 3.496710]\n",
      "159 [D loss: 0.350018, acc.: 87.50%] [G loss: 2.878919]\n",
      "160 [D loss: 0.143355, acc.: 96.88%] [G loss: 3.438194]\n",
      "161 [D loss: 0.882504, acc.: 67.19%] [G loss: 2.110136]\n",
      "162 [D loss: 0.178013, acc.: 90.62%] [G loss: 2.852975]\n",
      "163 [D loss: 0.303628, acc.: 87.50%] [G loss: 2.841530]\n",
      "164 [D loss: 0.246786, acc.: 89.06%] [G loss: 3.014622]\n",
      "165 [D loss: 0.202978, acc.: 95.31%] [G loss: 2.987085]\n",
      "166 [D loss: 0.207592, acc.: 90.62%] [G loss: 2.840888]\n",
      "167 [D loss: 0.229807, acc.: 89.06%] [G loss: 3.329019]\n",
      "168 [D loss: 0.214902, acc.: 92.19%] [G loss: 3.243167]\n",
      "169 [D loss: 0.144309, acc.: 100.00%] [G loss: 2.851062]\n",
      "170 [D loss: 0.327810, acc.: 79.69%] [G loss: 3.467728]\n",
      "171 [D loss: 0.332906, acc.: 81.25%] [G loss: 2.881777]\n",
      "172 [D loss: 0.171295, acc.: 95.31%] [G loss: 3.186627]\n",
      "173 [D loss: 0.358828, acc.: 79.69%] [G loss: 2.635366]\n",
      "174 [D loss: 0.118105, acc.: 96.88%] [G loss: 3.190653]\n",
      "175 [D loss: 0.204717, acc.: 96.88%] [G loss: 2.965102]\n",
      "176 [D loss: 0.205304, acc.: 89.06%] [G loss: 3.992228]\n",
      "177 [D loss: 0.765204, acc.: 57.81%] [G loss: 2.409582]\n",
      "178 [D loss: 0.128431, acc.: 95.31%] [G loss: 3.715304]\n",
      "179 [D loss: 0.190857, acc.: 96.88%] [G loss: 3.152881]\n",
      "180 [D loss: 0.153706, acc.: 96.88%] [G loss: 3.524242]\n",
      "181 [D loss: 0.190031, acc.: 96.88%] [G loss: 3.512696]\n",
      "182 [D loss: 0.140522, acc.: 100.00%] [G loss: 3.214361]\n",
      "183 [D loss: 0.261174, acc.: 90.62%] [G loss: 3.466527]\n",
      "184 [D loss: 0.204170, acc.: 92.19%] [G loss: 3.560259]\n",
      "185 [D loss: 0.263408, acc.: 90.62%] [G loss: 3.394884]\n",
      "186 [D loss: 0.175104, acc.: 95.31%] [G loss: 2.931543]\n",
      "187 [D loss: 0.173256, acc.: 95.31%] [G loss: 3.207534]\n",
      "188 [D loss: 0.171780, acc.: 96.88%] [G loss: 2.912024]\n",
      "189 [D loss: 0.117374, acc.: 95.31%] [G loss: 3.921475]\n",
      "190 [D loss: 0.603535, acc.: 73.44%] [G loss: 2.853680]\n",
      "191 [D loss: 0.148107, acc.: 93.75%] [G loss: 3.588156]\n",
      "192 [D loss: 0.166417, acc.: 96.88%] [G loss: 3.465839]\n",
      "193 [D loss: 0.217934, acc.: 90.62%] [G loss: 3.194084]\n",
      "194 [D loss: 0.102038, acc.: 100.00%] [G loss: 3.563599]\n",
      "195 [D loss: 0.252126, acc.: 92.19%] [G loss: 3.708204]\n",
      "196 [D loss: 0.559213, acc.: 71.88%] [G loss: 2.568850]\n",
      "197 [D loss: 0.162329, acc.: 93.75%] [G loss: 3.940223]\n",
      "198 [D loss: 0.515628, acc.: 78.12%] [G loss: 2.510097]\n",
      "199 [D loss: 0.256029, acc.: 84.38%] [G loss: 3.253807]\n",
      "200 [D loss: 0.095819, acc.: 98.44%] [G loss: 3.798357]\n",
      "201 [D loss: 0.202701, acc.: 93.75%] [G loss: 3.295786]\n",
      "202 [D loss: 0.163702, acc.: 95.31%] [G loss: 2.779567]\n",
      "203 [D loss: 0.264268, acc.: 87.50%] [G loss: 4.101708]\n",
      "204 [D loss: 0.523505, acc.: 68.75%] [G loss: 3.460145]\n",
      "205 [D loss: 0.124133, acc.: 100.00%] [G loss: 3.812726]\n",
      "206 [D loss: 0.208740, acc.: 93.75%] [G loss: 3.422639]\n",
      "207 [D loss: 0.226467, acc.: 87.50%] [G loss: 3.061695]\n",
      "208 [D loss: 0.241191, acc.: 90.62%] [G loss: 3.558785]\n",
      "209 [D loss: 0.266435, acc.: 85.94%] [G loss: 3.801381]\n",
      "210 [D loss: 0.453849, acc.: 73.44%] [G loss: 3.275863]\n",
      "211 [D loss: 0.251810, acc.: 89.06%] [G loss: 3.173107]\n",
      "212 [D loss: 0.345096, acc.: 85.94%] [G loss: 2.981136]\n",
      "213 [D loss: 0.172849, acc.: 92.19%] [G loss: 3.361637]\n",
      "214 [D loss: 0.394690, acc.: 79.69%] [G loss: 3.877825]\n",
      "215 [D loss: 0.230096, acc.: 92.19%] [G loss: 3.615064]\n",
      "216 [D loss: 0.453860, acc.: 78.12%] [G loss: 2.969151]\n",
      "217 [D loss: 0.451828, acc.: 75.00%] [G loss: 3.081906]\n",
      "218 [D loss: 0.272254, acc.: 92.19%] [G loss: 3.212759]\n",
      "219 [D loss: 0.541929, acc.: 68.75%] [G loss: 2.811674]\n",
      "220 [D loss: 0.242711, acc.: 89.06%] [G loss: 3.619107]\n",
      "221 [D loss: 1.176054, acc.: 45.31%] [G loss: 1.830047]\n",
      "222 [D loss: 0.182514, acc.: 92.19%] [G loss: 3.506649]\n",
      "223 [D loss: 0.688212, acc.: 59.38%] [G loss: 2.156558]\n",
      "224 [D loss: 0.194084, acc.: 95.31%] [G loss: 3.021739]\n",
      "225 [D loss: 0.322689, acc.: 87.50%] [G loss: 3.092193]\n",
      "226 [D loss: 0.321505, acc.: 85.94%] [G loss: 3.077619]\n",
      "227 [D loss: 0.271866, acc.: 92.19%] [G loss: 3.183767]\n",
      "228 [D loss: 0.621908, acc.: 67.19%] [G loss: 1.955147]\n",
      "229 [D loss: 0.191931, acc.: 92.19%] [G loss: 3.544950]\n",
      "230 [D loss: 0.913108, acc.: 43.75%] [G loss: 1.277801]\n",
      "231 [D loss: 0.495685, acc.: 70.31%] [G loss: 2.864561]\n",
      "232 [D loss: 0.207811, acc.: 90.62%] [G loss: 3.926816]\n",
      "233 [D loss: 0.951136, acc.: 48.44%] [G loss: 1.464749]\n",
      "234 [D loss: 0.372881, acc.: 81.25%] [G loss: 1.915780]\n",
      "235 [D loss: 0.229746, acc.: 95.31%] [G loss: 3.604084]\n",
      "236 [D loss: 0.667855, acc.: 60.94%] [G loss: 2.043704]\n",
      "237 [D loss: 0.251448, acc.: 89.06%] [G loss: 3.258132]\n",
      "238 [D loss: 0.607094, acc.: 67.19%] [G loss: 1.937621]\n",
      "239 [D loss: 0.324789, acc.: 82.81%] [G loss: 3.008215]\n",
      "240 [D loss: 0.450106, acc.: 78.12%] [G loss: 2.508831]\n",
      "241 [D loss: 0.332210, acc.: 87.50%] [G loss: 2.557696]\n",
      "242 [D loss: 0.360110, acc.: 82.81%] [G loss: 2.753930]\n",
      "243 [D loss: 0.565220, acc.: 70.31%] [G loss: 2.421823]\n",
      "244 [D loss: 0.361727, acc.: 81.25%] [G loss: 2.750122]\n",
      "245 [D loss: 0.349749, acc.: 89.06%] [G loss: 2.331494]\n",
      "246 [D loss: 0.425300, acc.: 78.12%] [G loss: 2.834930]\n",
      "247 [D loss: 0.471348, acc.: 75.00%] [G loss: 2.626553]\n",
      "248 [D loss: 0.716782, acc.: 62.50%] [G loss: 1.939413]\n",
      "249 [D loss: 0.376640, acc.: 78.12%] [G loss: 2.952367]\n",
      "250 [D loss: 0.642539, acc.: 67.19%] [G loss: 2.093907]\n",
      "251 [D loss: 0.361685, acc.: 81.25%] [G loss: 2.895611]\n",
      "252 [D loss: 0.762454, acc.: 54.69%] [G loss: 1.787594]\n",
      "253 [D loss: 0.364008, acc.: 81.25%] [G loss: 3.166858]\n",
      "254 [D loss: 0.912072, acc.: 53.12%] [G loss: 1.775551]\n",
      "255 [D loss: 0.304692, acc.: 85.94%] [G loss: 2.949767]\n",
      "256 [D loss: 0.516455, acc.: 78.12%] [G loss: 2.290229]\n",
      "257 [D loss: 0.377021, acc.: 81.25%] [G loss: 2.325218]\n",
      "258 [D loss: 0.481781, acc.: 79.69%] [G loss: 2.624601]\n",
      "259 [D loss: 0.682534, acc.: 60.94%] [G loss: 2.083603]\n",
      "260 [D loss: 0.433758, acc.: 79.69%] [G loss: 2.829993]\n",
      "261 [D loss: 0.599220, acc.: 59.38%] [G loss: 2.021812]\n",
      "262 [D loss: 0.423484, acc.: 76.56%] [G loss: 2.427680]\n",
      "263 [D loss: 0.685198, acc.: 65.62%] [G loss: 1.933707]\n",
      "264 [D loss: 0.425276, acc.: 79.69%] [G loss: 2.911636]\n",
      "265 [D loss: 0.765978, acc.: 53.12%] [G loss: 1.581236]\n",
      "266 [D loss: 0.420502, acc.: 81.25%] [G loss: 2.609545]\n",
      "267 [D loss: 0.664209, acc.: 59.38%] [G loss: 1.666556]\n",
      "268 [D loss: 0.555121, acc.: 75.00%] [G loss: 1.961738]\n",
      "269 [D loss: 0.462457, acc.: 78.12%] [G loss: 2.331856]\n",
      "270 [D loss: 0.771966, acc.: 53.12%] [G loss: 1.263224]\n",
      "271 [D loss: 0.506704, acc.: 65.62%] [G loss: 2.295835]\n",
      "272 [D loss: 0.716393, acc.: 51.56%] [G loss: 1.816888]\n",
      "273 [D loss: 0.577619, acc.: 67.19%] [G loss: 1.755012]\n",
      "274 [D loss: 0.571444, acc.: 65.62%] [G loss: 1.859462]\n",
      "275 [D loss: 0.546137, acc.: 73.44%] [G loss: 1.767809]\n",
      "276 [D loss: 0.658952, acc.: 60.94%] [G loss: 2.084628]\n",
      "277 [D loss: 0.627148, acc.: 65.62%] [G loss: 1.787983]\n",
      "278 [D loss: 0.558942, acc.: 65.62%] [G loss: 2.203366]\n",
      "279 [D loss: 0.676163, acc.: 57.81%] [G loss: 1.607224]\n",
      "280 [D loss: 0.574568, acc.: 65.62%] [G loss: 2.025175]\n",
      "281 [D loss: 0.533400, acc.: 70.31%] [G loss: 2.238969]\n",
      "282 [D loss: 0.751932, acc.: 48.44%] [G loss: 1.624144]\n",
      "283 [D loss: 0.565997, acc.: 70.31%] [G loss: 1.985259]\n",
      "284 [D loss: 0.520828, acc.: 75.00%] [G loss: 1.893441]\n",
      "285 [D loss: 0.769413, acc.: 53.12%] [G loss: 1.265038]\n",
      "286 [D loss: 0.508868, acc.: 70.31%] [G loss: 1.825620]\n",
      "287 [D loss: 0.724928, acc.: 54.69%] [G loss: 1.483024]\n",
      "288 [D loss: 0.532982, acc.: 73.44%] [G loss: 1.725120]\n",
      "289 [D loss: 0.547724, acc.: 71.88%] [G loss: 1.460065]\n",
      "290 [D loss: 0.572204, acc.: 73.44%] [G loss: 1.482407]\n",
      "291 [D loss: 0.559307, acc.: 71.88%] [G loss: 2.030184]\n",
      "292 [D loss: 0.899647, acc.: 40.62%] [G loss: 1.144067]\n",
      "293 [D loss: 0.624515, acc.: 59.38%] [G loss: 1.661813]\n",
      "294 [D loss: 0.831980, acc.: 48.44%] [G loss: 1.123530]\n",
      "295 [D loss: 0.632773, acc.: 57.81%] [G loss: 1.563972]\n",
      "296 [D loss: 0.662583, acc.: 57.81%] [G loss: 1.544444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 [D loss: 0.843540, acc.: 39.06%] [G loss: 0.995038]\n",
      "298 [D loss: 0.622171, acc.: 60.94%] [G loss: 1.513192]\n",
      "299 [D loss: 0.787911, acc.: 37.50%] [G loss: 1.079866]\n",
      "300 [D loss: 0.630210, acc.: 56.25%] [G loss: 1.397316]\n",
      "301 [D loss: 0.687848, acc.: 56.25%] [G loss: 1.179470]\n",
      "302 [D loss: 0.579277, acc.: 71.88%] [G loss: 1.473758]\n",
      "303 [D loss: 0.647292, acc.: 60.94%] [G loss: 1.195024]\n",
      "304 [D loss: 0.685287, acc.: 57.81%] [G loss: 1.283339]\n",
      "305 [D loss: 0.638916, acc.: 60.94%] [G loss: 1.271784]\n",
      "306 [D loss: 0.668066, acc.: 56.25%] [G loss: 1.174040]\n",
      "307 [D loss: 0.676945, acc.: 60.94%] [G loss: 1.188412]\n",
      "308 [D loss: 0.585641, acc.: 64.06%] [G loss: 1.065813]\n",
      "309 [D loss: 0.852167, acc.: 43.75%] [G loss: 0.836108]\n",
      "310 [D loss: 0.677417, acc.: 54.69%] [G loss: 1.012174]\n",
      "311 [D loss: 0.691543, acc.: 46.88%] [G loss: 1.175586]\n",
      "312 [D loss: 0.934452, acc.: 25.00%] [G loss: 0.741453]\n",
      "313 [D loss: 0.690563, acc.: 53.12%] [G loss: 0.960199]\n",
      "314 [D loss: 0.649495, acc.: 62.50%] [G loss: 1.160980]\n",
      "315 [D loss: 0.716641, acc.: 53.12%] [G loss: 1.003036]\n",
      "316 [D loss: 0.787621, acc.: 40.62%] [G loss: 0.797300]\n",
      "317 [D loss: 0.699127, acc.: 51.56%] [G loss: 0.896756]\n",
      "318 [D loss: 0.630871, acc.: 57.81%] [G loss: 1.063512]\n",
      "319 [D loss: 0.663058, acc.: 59.38%] [G loss: 1.000268]\n",
      "320 [D loss: 0.744623, acc.: 48.44%] [G loss: 0.821535]\n",
      "321 [D loss: 0.757333, acc.: 50.00%] [G loss: 0.902284]\n",
      "322 [D loss: 0.688007, acc.: 48.44%] [G loss: 0.901990]\n",
      "323 [D loss: 0.773311, acc.: 40.62%] [G loss: 0.862903]\n",
      "324 [D loss: 0.826456, acc.: 34.38%] [G loss: 0.670888]\n",
      "325 [D loss: 0.745801, acc.: 43.75%] [G loss: 0.706437]\n",
      "326 [D loss: 0.767721, acc.: 42.19%] [G loss: 0.710660]\n",
      "327 [D loss: 0.673582, acc.: 53.12%] [G loss: 0.750583]\n",
      "328 [D loss: 0.703056, acc.: 45.31%] [G loss: 0.788578]\n",
      "329 [D loss: 0.667931, acc.: 51.56%] [G loss: 0.861173]\n",
      "330 [D loss: 0.722728, acc.: 48.44%] [G loss: 0.798429]\n",
      "331 [D loss: 0.708766, acc.: 42.19%] [G loss: 0.726260]\n",
      "332 [D loss: 0.707257, acc.: 50.00%] [G loss: 0.783199]\n",
      "333 [D loss: 0.713841, acc.: 51.56%] [G loss: 0.774186]\n",
      "334 [D loss: 0.726057, acc.: 48.44%] [G loss: 0.754953]\n",
      "335 [D loss: 0.691106, acc.: 50.00%] [G loss: 0.807077]\n",
      "336 [D loss: 0.662822, acc.: 60.94%] [G loss: 0.797377]\n",
      "337 [D loss: 0.749752, acc.: 42.19%] [G loss: 0.737144]\n",
      "338 [D loss: 0.736387, acc.: 43.75%] [G loss: 0.691342]\n",
      "339 [D loss: 0.706463, acc.: 53.12%] [G loss: 0.696888]\n",
      "340 [D loss: 0.675011, acc.: 50.00%] [G loss: 0.730085]\n",
      "341 [D loss: 0.673096, acc.: 53.12%] [G loss: 0.765802]\n",
      "342 [D loss: 0.683583, acc.: 42.19%] [G loss: 0.779739]\n",
      "343 [D loss: 0.724513, acc.: 43.75%] [G loss: 0.718222]\n",
      "344 [D loss: 0.664847, acc.: 48.44%] [G loss: 0.729009]\n",
      "345 [D loss: 0.706046, acc.: 43.75%] [G loss: 0.736644]\n",
      "346 [D loss: 0.680912, acc.: 46.88%] [G loss: 0.735935]\n",
      "347 [D loss: 0.645279, acc.: 46.88%] [G loss: 0.763649]\n",
      "348 [D loss: 0.647464, acc.: 54.69%] [G loss: 0.774680]\n",
      "349 [D loss: 0.691269, acc.: 43.75%] [G loss: 0.758957]\n",
      "350 [D loss: 0.696714, acc.: 39.06%] [G loss: 0.724540]\n",
      "351 [D loss: 0.697910, acc.: 42.19%] [G loss: 0.721357]\n",
      "352 [D loss: 0.671988, acc.: 48.44%] [G loss: 0.707988]\n",
      "353 [D loss: 0.646960, acc.: 51.56%] [G loss: 0.741743]\n",
      "354 [D loss: 0.688255, acc.: 43.75%] [G loss: 0.766159]\n",
      "355 [D loss: 0.695373, acc.: 42.19%] [G loss: 0.728188]\n",
      "356 [D loss: 0.660388, acc.: 48.44%] [G loss: 0.735063]\n",
      "357 [D loss: 0.676918, acc.: 42.19%] [G loss: 0.742367]\n",
      "358 [D loss: 0.663611, acc.: 51.56%] [G loss: 0.737331]\n",
      "359 [D loss: 0.665655, acc.: 54.69%] [G loss: 0.739524]\n",
      "360 [D loss: 0.647750, acc.: 59.38%] [G loss: 0.766914]\n",
      "361 [D loss: 0.644913, acc.: 62.50%] [G loss: 0.768288]\n",
      "362 [D loss: 0.691726, acc.: 51.56%] [G loss: 0.735320]\n",
      "363 [D loss: 0.652977, acc.: 51.56%] [G loss: 0.713537]\n",
      "364 [D loss: 0.652967, acc.: 50.00%] [G loss: 0.719768]\n",
      "365 [D loss: 0.650414, acc.: 51.56%] [G loss: 0.708874]\n",
      "366 [D loss: 0.668511, acc.: 48.44%] [G loss: 0.715626]\n",
      "367 [D loss: 0.662501, acc.: 45.31%] [G loss: 0.740946]\n",
      "368 [D loss: 0.631047, acc.: 48.44%] [G loss: 0.752796]\n",
      "369 [D loss: 0.675529, acc.: 46.88%] [G loss: 0.703708]\n",
      "370 [D loss: 0.680547, acc.: 51.56%] [G loss: 0.714355]\n",
      "371 [D loss: 0.705327, acc.: 35.94%] [G loss: 0.684353]\n",
      "372 [D loss: 0.641713, acc.: 51.56%] [G loss: 0.710709]\n",
      "373 [D loss: 0.653185, acc.: 45.31%] [G loss: 0.766586]\n",
      "374 [D loss: 0.672007, acc.: 45.31%] [G loss: 0.753234]\n",
      "375 [D loss: 0.667779, acc.: 48.44%] [G loss: 0.722733]\n",
      "376 [D loss: 0.654385, acc.: 51.56%] [G loss: 0.716642]\n",
      "377 [D loss: 0.702771, acc.: 45.31%] [G loss: 0.698058]\n",
      "378 [D loss: 0.676748, acc.: 45.31%] [G loss: 0.768591]\n",
      "379 [D loss: 0.649453, acc.: 51.56%] [G loss: 0.776618]\n",
      "380 [D loss: 0.722025, acc.: 37.50%] [G loss: 0.762234]\n",
      "381 [D loss: 0.699300, acc.: 37.50%] [G loss: 0.747022]\n",
      "382 [D loss: 0.666753, acc.: 45.31%] [G loss: 0.721903]\n",
      "383 [D loss: 0.685549, acc.: 51.56%] [G loss: 0.719097]\n",
      "384 [D loss: 0.671419, acc.: 51.56%] [G loss: 0.735769]\n",
      "385 [D loss: 0.672941, acc.: 46.88%] [G loss: 0.765749]\n",
      "386 [D loss: 0.662598, acc.: 50.00%] [G loss: 0.741766]\n",
      "387 [D loss: 0.678680, acc.: 45.31%] [G loss: 0.723192]\n",
      "388 [D loss: 0.679077, acc.: 46.88%] [G loss: 0.754798]\n",
      "389 [D loss: 0.657674, acc.: 59.38%] [G loss: 0.763881]\n",
      "390 [D loss: 0.687698, acc.: 48.44%] [G loss: 0.756806]\n",
      "391 [D loss: 0.657810, acc.: 50.00%] [G loss: 0.748179]\n",
      "392 [D loss: 0.694682, acc.: 51.56%] [G loss: 0.725155]\n",
      "393 [D loss: 0.651679, acc.: 46.88%] [G loss: 0.755791]\n",
      "394 [D loss: 0.653068, acc.: 54.69%] [G loss: 0.802363]\n",
      "395 [D loss: 0.684398, acc.: 46.88%] [G loss: 0.789398]\n",
      "396 [D loss: 0.669821, acc.: 51.56%] [G loss: 0.803999]\n",
      "397 [D loss: 0.654696, acc.: 54.69%] [G loss: 0.787542]\n",
      "398 [D loss: 0.697372, acc.: 48.44%] [G loss: 0.744879]\n",
      "399 [D loss: 0.644464, acc.: 57.81%] [G loss: 0.728245]\n",
      "400 [D loss: 0.662522, acc.: 57.81%] [G loss: 0.735072]\n",
      "401 [D loss: 0.648382, acc.: 57.81%] [G loss: 0.749817]\n",
      "402 [D loss: 0.644394, acc.: 60.94%] [G loss: 0.767729]\n",
      "403 [D loss: 0.650976, acc.: 57.81%] [G loss: 0.779650]\n",
      "404 [D loss: 0.654733, acc.: 57.81%] [G loss: 0.798048]\n",
      "405 [D loss: 0.637527, acc.: 59.38%] [G loss: 0.762121]\n",
      "406 [D loss: 0.681291, acc.: 50.00%] [G loss: 0.753952]\n",
      "407 [D loss: 0.666264, acc.: 50.00%] [G loss: 0.735027]\n",
      "408 [D loss: 0.675926, acc.: 45.31%] [G loss: 0.723266]\n",
      "409 [D loss: 0.633413, acc.: 50.00%] [G loss: 0.731762]\n",
      "410 [D loss: 0.660496, acc.: 45.31%] [G loss: 0.720241]\n",
      "411 [D loss: 0.658774, acc.: 48.44%] [G loss: 0.721876]\n",
      "412 [D loss: 0.663789, acc.: 53.12%] [G loss: 0.734438]\n",
      "413 [D loss: 0.683211, acc.: 45.31%] [G loss: 0.746320]\n",
      "414 [D loss: 0.667389, acc.: 45.31%] [G loss: 0.733699]\n",
      "415 [D loss: 0.665807, acc.: 45.31%] [G loss: 0.742422]\n",
      "416 [D loss: 0.655347, acc.: 51.56%] [G loss: 0.742282]\n",
      "417 [D loss: 0.664457, acc.: 53.12%] [G loss: 0.731036]\n",
      "418 [D loss: 0.676402, acc.: 48.44%] [G loss: 0.744156]\n",
      "419 [D loss: 0.665311, acc.: 53.12%] [G loss: 0.775688]\n",
      "420 [D loss: 0.704310, acc.: 42.19%] [G loss: 0.766637]\n",
      "421 [D loss: 0.662291, acc.: 46.88%] [G loss: 0.749456]\n",
      "422 [D loss: 0.676449, acc.: 48.44%] [G loss: 0.744027]\n",
      "423 [D loss: 0.633445, acc.: 59.38%] [G loss: 0.716659]\n",
      "424 [D loss: 0.692304, acc.: 60.94%] [G loss: 0.714525]\n",
      "425 [D loss: 0.659118, acc.: 53.12%] [G loss: 0.757998]\n",
      "426 [D loss: 0.656264, acc.: 54.69%] [G loss: 0.748740]\n",
      "427 [D loss: 0.695539, acc.: 48.44%] [G loss: 0.690154]\n",
      "428 [D loss: 0.666170, acc.: 53.12%] [G loss: 0.693594]\n",
      "429 [D loss: 0.684680, acc.: 50.00%] [G loss: 0.707663]\n",
      "430 [D loss: 0.694752, acc.: 46.88%] [G loss: 0.747815]\n",
      "431 [D loss: 0.727600, acc.: 43.75%] [G loss: 0.757967]\n",
      "432 [D loss: 0.710106, acc.: 40.62%] [G loss: 0.706388]\n",
      "433 [D loss: 0.706932, acc.: 45.31%] [G loss: 0.710581]\n",
      "434 [D loss: 0.668031, acc.: 53.12%] [G loss: 0.696941]\n",
      "435 [D loss: 0.684949, acc.: 45.31%] [G loss: 0.739246]\n",
      "436 [D loss: 0.690138, acc.: 37.50%] [G loss: 0.699265]\n",
      "437 [D loss: 0.682425, acc.: 50.00%] [G loss: 0.708347]\n",
      "438 [D loss: 0.678092, acc.: 43.75%] [G loss: 0.725951]\n",
      "439 [D loss: 0.687414, acc.: 45.31%] [G loss: 0.714142]\n",
      "440 [D loss: 0.676615, acc.: 50.00%] [G loss: 0.732143]\n",
      "441 [D loss: 0.640616, acc.: 54.69%] [G loss: 0.744442]\n",
      "442 [D loss: 0.669238, acc.: 48.44%] [G loss: 0.771972]\n",
      "443 [D loss: 0.662155, acc.: 53.12%] [G loss: 0.732623]\n",
      "444 [D loss: 0.653224, acc.: 53.12%] [G loss: 0.726510]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445 [D loss: 0.677230, acc.: 45.31%] [G loss: 0.694951]\n",
      "446 [D loss: 0.653687, acc.: 50.00%] [G loss: 0.701994]\n",
      "447 [D loss: 0.651185, acc.: 46.88%] [G loss: 0.721748]\n",
      "448 [D loss: 0.649712, acc.: 50.00%] [G loss: 0.719996]\n",
      "449 [D loss: 0.658161, acc.: 50.00%] [G loss: 0.719654]\n",
      "450 [D loss: 0.642728, acc.: 51.56%] [G loss: 0.746638]\n",
      "451 [D loss: 0.626432, acc.: 53.12%] [G loss: 0.736306]\n",
      "452 [D loss: 0.615599, acc.: 56.25%] [G loss: 0.746634]\n",
      "453 [D loss: 0.631092, acc.: 54.69%] [G loss: 0.754494]\n",
      "454 [D loss: 0.630078, acc.: 62.50%] [G loss: 0.747873]\n",
      "455 [D loss: 0.603085, acc.: 64.06%] [G loss: 0.741145]\n",
      "456 [D loss: 0.605007, acc.: 64.06%] [G loss: 0.753742]\n",
      "457 [D loss: 0.621507, acc.: 59.38%] [G loss: 0.775674]\n",
      "458 [D loss: 0.660868, acc.: 51.56%] [G loss: 0.741692]\n",
      "459 [D loss: 0.641177, acc.: 50.00%] [G loss: 0.729628]\n",
      "460 [D loss: 0.633574, acc.: 54.69%] [G loss: 0.734864]\n",
      "461 [D loss: 0.676517, acc.: 51.56%] [G loss: 0.710771]\n",
      "462 [D loss: 0.617529, acc.: 56.25%] [G loss: 0.730037]\n",
      "463 [D loss: 0.639062, acc.: 59.38%] [G loss: 0.728729]\n",
      "464 [D loss: 0.635103, acc.: 53.12%] [G loss: 0.742669]\n",
      "465 [D loss: 0.638670, acc.: 56.25%] [G loss: 0.742026]\n",
      "466 [D loss: 0.663631, acc.: 54.69%] [G loss: 0.704429]\n",
      "467 [D loss: 0.638935, acc.: 53.12%] [G loss: 0.704557]\n",
      "468 [D loss: 0.640578, acc.: 53.12%] [G loss: 0.727725]\n",
      "469 [D loss: 0.642989, acc.: 59.38%] [G loss: 0.705886]\n",
      "470 [D loss: 0.639888, acc.: 53.12%] [G loss: 0.744680]\n",
      "471 [D loss: 0.650412, acc.: 51.56%] [G loss: 0.725167]\n",
      "472 [D loss: 0.665983, acc.: 51.56%] [G loss: 0.734290]\n",
      "473 [D loss: 0.629933, acc.: 62.50%] [G loss: 0.717401]\n",
      "474 [D loss: 0.653773, acc.: 50.00%] [G loss: 0.735590]\n",
      "475 [D loss: 0.635887, acc.: 60.94%] [G loss: 0.714285]\n",
      "476 [D loss: 0.628379, acc.: 62.50%] [G loss: 0.721970]\n",
      "477 [D loss: 0.660555, acc.: 56.25%] [G loss: 0.750196]\n",
      "478 [D loss: 0.674607, acc.: 48.44%] [G loss: 0.705226]\n",
      "479 [D loss: 0.678206, acc.: 46.88%] [G loss: 0.675704]\n",
      "480 [D loss: 0.689093, acc.: 54.69%] [G loss: 0.708068]\n",
      "481 [D loss: 0.651306, acc.: 54.69%] [G loss: 0.706261]\n",
      "482 [D loss: 0.649873, acc.: 56.25%] [G loss: 0.707192]\n",
      "483 [D loss: 0.620366, acc.: 62.50%] [G loss: 0.753703]\n",
      "484 [D loss: 0.662787, acc.: 56.25%] [G loss: 0.724016]\n",
      "485 [D loss: 0.650797, acc.: 57.81%] [G loss: 0.704992]\n",
      "486 [D loss: 0.638568, acc.: 60.94%] [G loss: 0.721605]\n",
      "487 [D loss: 0.656360, acc.: 59.38%] [G loss: 0.749529]\n",
      "488 [D loss: 0.660433, acc.: 60.94%] [G loss: 0.726655]\n",
      "489 [D loss: 0.658001, acc.: 50.00%] [G loss: 0.733505]\n",
      "490 [D loss: 0.632602, acc.: 67.19%] [G loss: 0.762074]\n",
      "491 [D loss: 0.646541, acc.: 60.94%] [G loss: 0.739576]\n",
      "492 [D loss: 0.650367, acc.: 62.50%] [G loss: 0.706853]\n",
      "493 [D loss: 0.657812, acc.: 54.69%] [G loss: 0.707322]\n",
      "494 [D loss: 0.625383, acc.: 68.75%] [G loss: 0.716660]\n",
      "495 [D loss: 0.654293, acc.: 56.25%] [G loss: 0.692115]\n",
      "496 [D loss: 0.672740, acc.: 54.69%] [G loss: 0.718760]\n",
      "497 [D loss: 0.665820, acc.: 59.38%] [G loss: 0.715278]\n",
      "498 [D loss: 0.671828, acc.: 57.81%] [G loss: 0.724201]\n",
      "499 [D loss: 0.647552, acc.: 56.25%] [G loss: 0.689342]\n",
      "500 [D loss: 0.665466, acc.: 59.38%] [G loss: 0.709224]\n",
      "501 [D loss: 0.667596, acc.: 59.38%] [G loss: 0.754298]\n",
      "502 [D loss: 0.647043, acc.: 60.94%] [G loss: 0.749271]\n",
      "503 [D loss: 0.651215, acc.: 60.94%] [G loss: 0.734746]\n",
      "504 [D loss: 0.649653, acc.: 56.25%] [G loss: 0.733883]\n",
      "505 [D loss: 0.660058, acc.: 57.81%] [G loss: 0.736846]\n",
      "506 [D loss: 0.651868, acc.: 56.25%] [G loss: 0.734513]\n",
      "507 [D loss: 0.640801, acc.: 62.50%] [G loss: 0.734534]\n",
      "508 [D loss: 0.670906, acc.: 57.81%] [G loss: 0.718547]\n",
      "509 [D loss: 0.658269, acc.: 57.81%] [G loss: 0.721430]\n",
      "510 [D loss: 0.640809, acc.: 67.19%] [G loss: 0.697199]\n",
      "511 [D loss: 0.664032, acc.: 54.69%] [G loss: 0.703028]\n",
      "512 [D loss: 0.716971, acc.: 46.88%] [G loss: 0.706130]\n",
      "513 [D loss: 0.673377, acc.: 50.00%] [G loss: 0.728094]\n",
      "514 [D loss: 0.665067, acc.: 51.56%] [G loss: 0.759500]\n",
      "515 [D loss: 0.689643, acc.: 53.12%] [G loss: 0.743894]\n",
      "516 [D loss: 0.671136, acc.: 51.56%] [G loss: 0.743841]\n",
      "517 [D loss: 0.672308, acc.: 50.00%] [G loss: 0.714163]\n",
      "518 [D loss: 0.685661, acc.: 54.69%] [G loss: 0.698622]\n",
      "519 [D loss: 0.698202, acc.: 43.75%] [G loss: 0.692654]\n",
      "520 [D loss: 0.681367, acc.: 54.69%] [G loss: 0.732289]\n",
      "521 [D loss: 0.642535, acc.: 56.25%] [G loss: 0.728859]\n",
      "522 [D loss: 0.679636, acc.: 42.19%] [G loss: 0.723301]\n",
      "523 [D loss: 0.682170, acc.: 50.00%] [G loss: 0.742781]\n",
      "524 [D loss: 0.669332, acc.: 48.44%] [G loss: 0.733898]\n",
      "525 [D loss: 0.655025, acc.: 54.69%] [G loss: 0.746212]\n",
      "526 [D loss: 0.670972, acc.: 50.00%] [G loss: 0.729960]\n",
      "527 [D loss: 0.661338, acc.: 51.56%] [G loss: 0.717365]\n",
      "528 [D loss: 0.670328, acc.: 46.88%] [G loss: 0.708539]\n",
      "529 [D loss: 0.653146, acc.: 46.88%] [G loss: 0.724215]\n",
      "530 [D loss: 0.688917, acc.: 50.00%] [G loss: 0.724469]\n",
      "531 [D loss: 0.662336, acc.: 48.44%] [G loss: 0.734686]\n",
      "532 [D loss: 0.669874, acc.: 56.25%] [G loss: 0.747619]\n",
      "533 [D loss: 0.658675, acc.: 56.25%] [G loss: 0.742716]\n",
      "534 [D loss: 0.674694, acc.: 50.00%] [G loss: 0.743155]\n",
      "535 [D loss: 0.651816, acc.: 48.44%] [G loss: 0.760123]\n",
      "536 [D loss: 0.655485, acc.: 51.56%] [G loss: 0.753550]\n",
      "537 [D loss: 0.650796, acc.: 54.69%] [G loss: 0.753134]\n",
      "538 [D loss: 0.638412, acc.: 56.25%] [G loss: 0.753329]\n",
      "539 [D loss: 0.654725, acc.: 54.69%] [G loss: 0.753001]\n",
      "540 [D loss: 0.653743, acc.: 56.25%] [G loss: 0.762258]\n",
      "541 [D loss: 0.650511, acc.: 48.44%] [G loss: 0.762346]\n",
      "542 [D loss: 0.658814, acc.: 50.00%] [G loss: 0.747381]\n",
      "543 [D loss: 0.647349, acc.: 54.69%] [G loss: 0.728630]\n",
      "544 [D loss: 0.642404, acc.: 51.56%] [G loss: 0.738983]\n",
      "545 [D loss: 0.640338, acc.: 56.25%] [G loss: 0.731062]\n",
      "546 [D loss: 0.627057, acc.: 59.38%] [G loss: 0.732268]\n",
      "547 [D loss: 0.641818, acc.: 60.94%] [G loss: 0.740695]\n",
      "548 [D loss: 0.613785, acc.: 65.62%] [G loss: 0.742357]\n",
      "549 [D loss: 0.645564, acc.: 59.38%] [G loss: 0.755017]\n",
      "550 [D loss: 0.645926, acc.: 57.81%] [G loss: 0.757712]\n",
      "551 [D loss: 0.631097, acc.: 62.50%] [G loss: 0.777115]\n",
      "552 [D loss: 0.676561, acc.: 51.56%] [G loss: 0.763856]\n",
      "553 [D loss: 0.673132, acc.: 48.44%] [G loss: 0.731921]\n",
      "554 [D loss: 0.630089, acc.: 59.38%] [G loss: 0.749197]\n",
      "555 [D loss: 0.627606, acc.: 56.25%] [G loss: 0.757677]\n",
      "556 [D loss: 0.651539, acc.: 57.81%] [G loss: 0.759069]\n",
      "557 [D loss: 0.630196, acc.: 65.62%] [G loss: 0.733228]\n",
      "558 [D loss: 0.627935, acc.: 68.75%] [G loss: 0.754842]\n",
      "559 [D loss: 0.669057, acc.: 56.25%] [G loss: 0.738969]\n",
      "560 [D loss: 0.643370, acc.: 64.06%] [G loss: 0.735902]\n",
      "561 [D loss: 0.664615, acc.: 53.12%] [G loss: 0.731772]\n",
      "562 [D loss: 0.628260, acc.: 62.50%] [G loss: 0.786287]\n",
      "563 [D loss: 0.625074, acc.: 68.75%] [G loss: 0.777591]\n",
      "564 [D loss: 0.669990, acc.: 59.38%] [G loss: 0.772466]\n",
      "565 [D loss: 0.647448, acc.: 67.19%] [G loss: 0.770263]\n",
      "566 [D loss: 0.644269, acc.: 65.62%] [G loss: 0.765675]\n",
      "567 [D loss: 0.629484, acc.: 67.19%] [G loss: 0.782020]\n",
      "568 [D loss: 0.668595, acc.: 54.69%] [G loss: 0.753312]\n",
      "569 [D loss: 0.609465, acc.: 68.75%] [G loss: 0.734608]\n",
      "570 [D loss: 0.626286, acc.: 62.50%] [G loss: 0.720966]\n",
      "571 [D loss: 0.633253, acc.: 67.19%] [G loss: 0.760373]\n",
      "572 [D loss: 0.608539, acc.: 75.00%] [G loss: 0.805042]\n",
      "573 [D loss: 0.621275, acc.: 67.19%] [G loss: 0.787391]\n",
      "574 [D loss: 0.677492, acc.: 59.38%] [G loss: 0.752254]\n",
      "575 [D loss: 0.649267, acc.: 57.81%] [G loss: 0.758474]\n",
      "576 [D loss: 0.683686, acc.: 50.00%] [G loss: 0.770457]\n",
      "577 [D loss: 0.658646, acc.: 53.12%] [G loss: 0.773006]\n",
      "578 [D loss: 0.694867, acc.: 50.00%] [G loss: 0.756550]\n",
      "579 [D loss: 0.630411, acc.: 62.50%] [G loss: 0.766754]\n",
      "580 [D loss: 0.672793, acc.: 50.00%] [G loss: 0.737301]\n",
      "581 [D loss: 0.656188, acc.: 50.00%] [G loss: 0.721193]\n",
      "582 [D loss: 0.665272, acc.: 48.44%] [G loss: 0.721697]\n",
      "583 [D loss: 0.652879, acc.: 54.69%] [G loss: 0.722326]\n",
      "584 [D loss: 0.657695, acc.: 48.44%] [G loss: 0.709524]\n",
      "585 [D loss: 0.635476, acc.: 57.81%] [G loss: 0.730666]\n",
      "586 [D loss: 0.653734, acc.: 54.69%] [G loss: 0.730059]\n",
      "587 [D loss: 0.618879, acc.: 65.62%] [G loss: 0.765375]\n",
      "588 [D loss: 0.643482, acc.: 59.38%] [G loss: 0.790754]\n",
      "589 [D loss: 0.660207, acc.: 51.56%] [G loss: 0.759054]\n",
      "590 [D loss: 0.620633, acc.: 67.19%] [G loss: 0.758377]\n",
      "591 [D loss: 0.668446, acc.: 57.81%] [G loss: 0.761236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592 [D loss: 0.652238, acc.: 57.81%] [G loss: 0.753224]\n",
      "593 [D loss: 0.670662, acc.: 50.00%] [G loss: 0.726719]\n",
      "594 [D loss: 0.643540, acc.: 62.50%] [G loss: 0.735327]\n",
      "595 [D loss: 0.629363, acc.: 60.94%] [G loss: 0.764542]\n",
      "596 [D loss: 0.649915, acc.: 59.38%] [G loss: 0.739950]\n",
      "597 [D loss: 0.661535, acc.: 59.38%] [G loss: 0.770043]\n",
      "598 [D loss: 0.622566, acc.: 62.50%] [G loss: 0.784754]\n",
      "599 [D loss: 0.644223, acc.: 56.25%] [G loss: 0.771602]\n",
      "600 [D loss: 0.637295, acc.: 60.94%] [G loss: 0.782759]\n",
      "601 [D loss: 0.661038, acc.: 51.56%] [G loss: 0.775782]\n",
      "602 [D loss: 0.611352, acc.: 71.88%] [G loss: 0.775686]\n",
      "603 [D loss: 0.643964, acc.: 68.75%] [G loss: 0.761788]\n",
      "604 [D loss: 0.646055, acc.: 57.81%] [G loss: 0.767645]\n",
      "605 [D loss: 0.649958, acc.: 54.69%] [G loss: 0.776400]\n",
      "606 [D loss: 0.635963, acc.: 60.94%] [G loss: 0.798469]\n",
      "607 [D loss: 0.682827, acc.: 46.88%] [G loss: 0.762610]\n",
      "608 [D loss: 0.637136, acc.: 59.38%] [G loss: 0.757759]\n",
      "609 [D loss: 0.632424, acc.: 56.25%] [G loss: 0.750715]\n",
      "610 [D loss: 0.649935, acc.: 60.94%] [G loss: 0.780134]\n",
      "611 [D loss: 0.620501, acc.: 67.19%] [G loss: 0.751832]\n",
      "612 [D loss: 0.645827, acc.: 59.38%] [G loss: 0.751482]\n",
      "613 [D loss: 0.666862, acc.: 57.81%] [G loss: 0.757242]\n",
      "614 [D loss: 0.646060, acc.: 68.75%] [G loss: 0.752477]\n",
      "615 [D loss: 0.633949, acc.: 59.38%] [G loss: 0.779978]\n",
      "616 [D loss: 0.625280, acc.: 75.00%] [G loss: 0.783601]\n",
      "617 [D loss: 0.617668, acc.: 64.06%] [G loss: 0.786461]\n",
      "618 [D loss: 0.640443, acc.: 60.94%] [G loss: 0.777134]\n",
      "619 [D loss: 0.644841, acc.: 64.06%] [G loss: 0.755560]\n",
      "620 [D loss: 0.652340, acc.: 64.06%] [G loss: 0.726568]\n",
      "621 [D loss: 0.656862, acc.: 57.81%] [G loss: 0.745710]\n",
      "622 [D loss: 0.653410, acc.: 64.06%] [G loss: 0.740516]\n",
      "623 [D loss: 0.640124, acc.: 70.31%] [G loss: 0.735153]\n",
      "624 [D loss: 0.634314, acc.: 75.00%] [G loss: 0.749742]\n",
      "625 [D loss: 0.638429, acc.: 71.88%] [G loss: 0.764141]\n",
      "626 [D loss: 0.648149, acc.: 70.31%] [G loss: 0.749573]\n",
      "627 [D loss: 0.600252, acc.: 79.69%] [G loss: 0.748982]\n",
      "628 [D loss: 0.658963, acc.: 67.19%] [G loss: 0.780259]\n",
      "629 [D loss: 0.620160, acc.: 71.88%] [G loss: 0.809357]\n",
      "630 [D loss: 0.621590, acc.: 71.88%] [G loss: 0.811117]\n",
      "631 [D loss: 0.628436, acc.: 70.31%] [G loss: 0.781156]\n",
      "632 [D loss: 0.648809, acc.: 62.50%] [G loss: 0.752491]\n",
      "633 [D loss: 0.619265, acc.: 68.75%] [G loss: 0.752388]\n",
      "634 [D loss: 0.656285, acc.: 59.38%] [G loss: 0.742665]\n",
      "635 [D loss: 0.632359, acc.: 54.69%] [G loss: 0.775671]\n",
      "636 [D loss: 0.636924, acc.: 64.06%] [G loss: 0.798147]\n",
      "637 [D loss: 0.663315, acc.: 56.25%] [G loss: 0.746994]\n",
      "638 [D loss: 0.641184, acc.: 60.94%] [G loss: 0.738048]\n",
      "639 [D loss: 0.636225, acc.: 59.38%] [G loss: 0.749317]\n",
      "640 [D loss: 0.633256, acc.: 57.81%] [G loss: 0.761978]\n",
      "641 [D loss: 0.651490, acc.: 59.38%] [G loss: 0.740277]\n",
      "642 [D loss: 0.635685, acc.: 68.75%] [G loss: 0.783410]\n",
      "643 [D loss: 0.640770, acc.: 64.06%] [G loss: 0.758784]\n",
      "644 [D loss: 0.616575, acc.: 70.31%] [G loss: 0.779246]\n",
      "645 [D loss: 0.615054, acc.: 65.62%] [G loss: 0.778007]\n",
      "646 [D loss: 0.651027, acc.: 64.06%] [G loss: 0.744958]\n",
      "647 [D loss: 0.652982, acc.: 60.94%] [G loss: 0.774792]\n",
      "648 [D loss: 0.640307, acc.: 62.50%] [G loss: 0.766189]\n",
      "649 [D loss: 0.643264, acc.: 57.81%] [G loss: 0.769650]\n",
      "650 [D loss: 0.633983, acc.: 62.50%] [G loss: 0.756738]\n",
      "651 [D loss: 0.640048, acc.: 65.62%] [G loss: 0.727807]\n",
      "652 [D loss: 0.651846, acc.: 59.38%] [G loss: 0.682900]\n",
      "653 [D loss: 0.643708, acc.: 57.81%] [G loss: 0.679914]\n",
      "654 [D loss: 0.651923, acc.: 57.81%] [G loss: 0.670572]\n",
      "655 [D loss: 0.647356, acc.: 64.06%] [G loss: 0.710436]\n",
      "656 [D loss: 0.660922, acc.: 60.94%] [G loss: 0.739347]\n",
      "657 [D loss: 0.638959, acc.: 68.75%] [G loss: 0.794884]\n",
      "658 [D loss: 0.633421, acc.: 65.62%] [G loss: 0.791336]\n",
      "659 [D loss: 0.680631, acc.: 54.69%] [G loss: 0.782546]\n",
      "660 [D loss: 0.664157, acc.: 50.00%] [G loss: 0.827097]\n",
      "661 [D loss: 0.659541, acc.: 57.81%] [G loss: 0.836794]\n",
      "662 [D loss: 0.630068, acc.: 62.50%] [G loss: 0.874967]\n",
      "663 [D loss: 0.638492, acc.: 62.50%] [G loss: 0.868515]\n",
      "664 [D loss: 0.654838, acc.: 56.25%] [G loss: 0.827366]\n",
      "665 [D loss: 0.626635, acc.: 65.62%] [G loss: 0.824912]\n",
      "666 [D loss: 0.675295, acc.: 51.56%] [G loss: 0.801963]\n",
      "667 [D loss: 0.631651, acc.: 56.25%] [G loss: 0.775748]\n",
      "668 [D loss: 0.677906, acc.: 48.44%] [G loss: 0.740071]\n",
      "669 [D loss: 0.651001, acc.: 50.00%] [G loss: 0.751309]\n",
      "670 [D loss: 0.629282, acc.: 59.38%] [G loss: 0.778443]\n",
      "671 [D loss: 0.648356, acc.: 54.69%] [G loss: 0.761962]\n",
      "672 [D loss: 0.651606, acc.: 57.81%] [G loss: 0.786648]\n",
      "673 [D loss: 0.671880, acc.: 51.56%] [G loss: 0.766948]\n",
      "674 [D loss: 0.657207, acc.: 53.12%] [G loss: 0.755405]\n",
      "675 [D loss: 0.629575, acc.: 59.38%] [G loss: 0.773715]\n",
      "676 [D loss: 0.627477, acc.: 64.06%] [G loss: 0.785864]\n",
      "677 [D loss: 0.645892, acc.: 59.38%] [G loss: 0.822615]\n",
      "678 [D loss: 0.651746, acc.: 59.38%] [G loss: 0.775951]\n",
      "679 [D loss: 0.632887, acc.: 62.50%] [G loss: 0.758146]\n",
      "680 [D loss: 0.635527, acc.: 60.94%] [G loss: 0.750624]\n",
      "681 [D loss: 0.613979, acc.: 64.06%] [G loss: 0.774334]\n",
      "682 [D loss: 0.619079, acc.: 65.62%] [G loss: 0.780318]\n",
      "683 [D loss: 0.646541, acc.: 57.81%] [G loss: 0.794353]\n",
      "684 [D loss: 0.634799, acc.: 60.94%] [G loss: 0.763543]\n",
      "685 [D loss: 0.630887, acc.: 59.38%] [G loss: 0.783372]\n",
      "686 [D loss: 0.627524, acc.: 64.06%] [G loss: 0.766894]\n",
      "687 [D loss: 0.659776, acc.: 54.69%] [G loss: 0.794719]\n",
      "688 [D loss: 0.629929, acc.: 64.06%] [G loss: 0.789789]\n",
      "689 [D loss: 0.634300, acc.: 65.62%] [G loss: 0.819397]\n",
      "690 [D loss: 0.611339, acc.: 70.31%] [G loss: 0.766140]\n",
      "691 [D loss: 0.708867, acc.: 45.31%] [G loss: 0.752931]\n",
      "692 [D loss: 0.641761, acc.: 64.06%] [G loss: 0.747915]\n",
      "693 [D loss: 0.632456, acc.: 48.44%] [G loss: 0.816940]\n",
      "694 [D loss: 0.616319, acc.: 67.19%] [G loss: 0.816042]\n",
      "695 [D loss: 0.657146, acc.: 65.62%] [G loss: 0.799228]\n",
      "696 [D loss: 0.635036, acc.: 68.75%] [G loss: 0.779181]\n",
      "697 [D loss: 0.651922, acc.: 64.06%] [G loss: 0.751461]\n",
      "698 [D loss: 0.641761, acc.: 62.50%] [G loss: 0.747954]\n",
      "699 [D loss: 0.625453, acc.: 64.06%] [G loss: 0.758305]\n",
      "700 [D loss: 0.636650, acc.: 57.81%] [G loss: 0.788857]\n",
      "701 [D loss: 0.648129, acc.: 68.75%] [G loss: 0.763296]\n",
      "702 [D loss: 0.665053, acc.: 65.62%] [G loss: 0.804675]\n",
      "703 [D loss: 0.623538, acc.: 75.00%] [G loss: 0.770804]\n",
      "704 [D loss: 0.639290, acc.: 60.94%] [G loss: 0.757006]\n",
      "705 [D loss: 0.614059, acc.: 68.75%] [G loss: 0.796540]\n",
      "706 [D loss: 0.626625, acc.: 65.62%] [G loss: 0.772837]\n",
      "707 [D loss: 0.601187, acc.: 68.75%] [G loss: 0.788870]\n",
      "708 [D loss: 0.586834, acc.: 71.88%] [G loss: 0.798180]\n",
      "709 [D loss: 0.587505, acc.: 75.00%] [G loss: 0.836376]\n",
      "710 [D loss: 0.645580, acc.: 64.06%] [G loss: 0.854572]\n",
      "711 [D loss: 0.606595, acc.: 78.12%] [G loss: 0.836196]\n",
      "712 [D loss: 0.633714, acc.: 71.88%] [G loss: 0.806948]\n",
      "713 [D loss: 0.604647, acc.: 70.31%] [G loss: 0.803261]\n",
      "714 [D loss: 0.625791, acc.: 64.06%] [G loss: 0.783334]\n",
      "715 [D loss: 0.610440, acc.: 60.94%] [G loss: 0.776088]\n",
      "716 [D loss: 0.576514, acc.: 79.69%] [G loss: 0.822335]\n",
      "717 [D loss: 0.592168, acc.: 73.44%] [G loss: 0.778208]\n",
      "718 [D loss: 0.594626, acc.: 79.69%] [G loss: 0.792284]\n",
      "719 [D loss: 0.637574, acc.: 64.06%] [G loss: 0.784202]\n",
      "720 [D loss: 0.576132, acc.: 73.44%] [G loss: 0.799418]\n",
      "721 [D loss: 0.610110, acc.: 76.56%] [G loss: 0.770277]\n",
      "722 [D loss: 0.596370, acc.: 71.88%] [G loss: 0.804283]\n",
      "723 [D loss: 0.622835, acc.: 75.00%] [G loss: 0.790036]\n",
      "724 [D loss: 0.579235, acc.: 79.69%] [G loss: 0.802106]\n",
      "725 [D loss: 0.628049, acc.: 67.19%] [G loss: 0.795171]\n",
      "726 [D loss: 0.620050, acc.: 57.81%] [G loss: 0.788969]\n",
      "727 [D loss: 0.627430, acc.: 64.06%] [G loss: 0.781544]\n",
      "728 [D loss: 0.635092, acc.: 62.50%] [G loss: 0.746886]\n",
      "729 [D loss: 0.631074, acc.: 65.62%] [G loss: 0.773654]\n",
      "730 [D loss: 0.608229, acc.: 64.06%] [G loss: 0.766981]\n",
      "731 [D loss: 0.595685, acc.: 67.19%] [G loss: 0.786571]\n",
      "732 [D loss: 0.622714, acc.: 68.75%] [G loss: 0.797771]\n",
      "733 [D loss: 0.623937, acc.: 67.19%] [G loss: 0.786903]\n",
      "734 [D loss: 0.615736, acc.: 71.88%] [G loss: 0.778934]\n",
      "735 [D loss: 0.611807, acc.: 59.38%] [G loss: 0.786959]\n",
      "736 [D loss: 0.632571, acc.: 59.38%] [G loss: 0.819623]\n",
      "737 [D loss: 0.600525, acc.: 70.31%] [G loss: 0.813159]\n",
      "738 [D loss: 0.624580, acc.: 65.62%] [G loss: 0.793015]\n",
      "739 [D loss: 0.630327, acc.: 62.50%] [G loss: 0.815565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740 [D loss: 0.627298, acc.: 65.62%] [G loss: 0.821568]\n",
      "741 [D loss: 0.598128, acc.: 71.88%] [G loss: 0.813590]\n",
      "742 [D loss: 0.609303, acc.: 68.75%] [G loss: 0.822433]\n",
      "743 [D loss: 0.608781, acc.: 65.62%] [G loss: 0.822203]\n",
      "744 [D loss: 0.618708, acc.: 67.19%] [G loss: 0.833003]\n",
      "745 [D loss: 0.639489, acc.: 60.94%] [G loss: 0.831568]\n",
      "746 [D loss: 0.606726, acc.: 67.19%] [G loss: 0.834782]\n",
      "747 [D loss: 0.611519, acc.: 71.88%] [G loss: 0.814656]\n",
      "748 [D loss: 0.565677, acc.: 81.25%] [G loss: 0.811943]\n",
      "749 [D loss: 0.623799, acc.: 59.38%] [G loss: 0.812000]\n",
      "750 [D loss: 0.610264, acc.: 65.62%] [G loss: 0.805086]\n",
      "751 [D loss: 0.624873, acc.: 57.81%] [G loss: 0.822593]\n",
      "752 [D loss: 0.634506, acc.: 64.06%] [G loss: 0.827058]\n",
      "753 [D loss: 0.617995, acc.: 71.88%] [G loss: 0.860390]\n",
      "754 [D loss: 0.653347, acc.: 57.81%] [G loss: 0.844799]\n",
      "755 [D loss: 0.594134, acc.: 73.44%] [G loss: 0.824087]\n",
      "756 [D loss: 0.617424, acc.: 68.75%] [G loss: 0.808748]\n",
      "757 [D loss: 0.617346, acc.: 60.94%] [G loss: 0.811736]\n",
      "758 [D loss: 0.600353, acc.: 64.06%] [G loss: 0.813150]\n",
      "759 [D loss: 0.573725, acc.: 70.31%] [G loss: 0.859038]\n",
      "760 [D loss: 0.630193, acc.: 65.62%] [G loss: 0.833385]\n",
      "761 [D loss: 0.614960, acc.: 65.62%] [G loss: 0.789111]\n",
      "762 [D loss: 0.613503, acc.: 70.31%] [G loss: 0.770654]\n",
      "763 [D loss: 0.641192, acc.: 56.25%] [G loss: 0.823106]\n",
      "764 [D loss: 0.581111, acc.: 78.12%] [G loss: 0.832745]\n",
      "765 [D loss: 0.630885, acc.: 60.94%] [G loss: 0.839230]\n",
      "766 [D loss: 0.628755, acc.: 79.69%] [G loss: 0.862417]\n",
      "767 [D loss: 0.596999, acc.: 71.88%] [G loss: 0.866290]\n",
      "768 [D loss: 0.614133, acc.: 75.00%] [G loss: 0.830636]\n",
      "769 [D loss: 0.619649, acc.: 65.62%] [G loss: 0.839941]\n",
      "770 [D loss: 0.636522, acc.: 60.94%] [G loss: 0.824934]\n",
      "771 [D loss: 0.616620, acc.: 60.94%] [G loss: 0.892659]\n",
      "772 [D loss: 0.597097, acc.: 70.31%] [G loss: 0.883969]\n",
      "773 [D loss: 0.672562, acc.: 56.25%] [G loss: 0.786022]\n",
      "774 [D loss: 0.624186, acc.: 59.38%] [G loss: 0.819268]\n",
      "775 [D loss: 0.616051, acc.: 68.75%] [G loss: 0.831959]\n",
      "776 [D loss: 0.656966, acc.: 60.94%] [G loss: 0.876036]\n",
      "777 [D loss: 0.626833, acc.: 68.75%] [G loss: 0.843834]\n",
      "778 [D loss: 0.649908, acc.: 54.69%] [G loss: 0.821001]\n",
      "779 [D loss: 0.629025, acc.: 65.62%] [G loss: 0.862054]\n",
      "780 [D loss: 0.683038, acc.: 54.69%] [G loss: 0.780402]\n",
      "781 [D loss: 0.674288, acc.: 48.44%] [G loss: 0.786203]\n",
      "782 [D loss: 0.649204, acc.: 57.81%] [G loss: 0.763030]\n",
      "783 [D loss: 0.671345, acc.: 57.81%] [G loss: 0.778262]\n",
      "784 [D loss: 0.663907, acc.: 57.81%] [G loss: 0.741925]\n",
      "785 [D loss: 0.643605, acc.: 62.50%] [G loss: 0.750223]\n",
      "786 [D loss: 0.645390, acc.: 59.38%] [G loss: 0.760337]\n",
      "787 [D loss: 0.650870, acc.: 64.06%] [G loss: 0.797700]\n",
      "788 [D loss: 0.621660, acc.: 62.50%] [G loss: 0.856178]\n",
      "789 [D loss: 0.627797, acc.: 67.19%] [G loss: 0.820125]\n",
      "790 [D loss: 0.653230, acc.: 67.19%] [G loss: 0.739134]\n",
      "791 [D loss: 0.621883, acc.: 71.88%] [G loss: 0.812020]\n",
      "792 [D loss: 0.620578, acc.: 68.75%] [G loss: 0.794535]\n",
      "793 [D loss: 0.614004, acc.: 65.62%] [G loss: 0.803012]\n",
      "794 [D loss: 0.619511, acc.: 73.44%] [G loss: 0.797077]\n",
      "795 [D loss: 0.603916, acc.: 70.31%] [G loss: 0.783802]\n",
      "796 [D loss: 0.609380, acc.: 75.00%] [G loss: 0.801680]\n",
      "797 [D loss: 0.585927, acc.: 76.56%] [G loss: 0.787320]\n",
      "798 [D loss: 0.647992, acc.: 60.94%] [G loss: 0.787070]\n",
      "799 [D loss: 0.630692, acc.: 65.62%] [G loss: 0.777320]\n",
      "800 [D loss: 0.636461, acc.: 60.94%] [G loss: 0.784261]\n",
      "801 [D loss: 0.599939, acc.: 70.31%] [G loss: 0.774638]\n",
      "802 [D loss: 0.672692, acc.: 56.25%] [G loss: 0.794925]\n",
      "803 [D loss: 0.611705, acc.: 71.88%] [G loss: 0.846019]\n",
      "804 [D loss: 0.659912, acc.: 59.38%] [G loss: 0.851703]\n",
      "805 [D loss: 0.643203, acc.: 67.19%] [G loss: 0.817755]\n",
      "806 [D loss: 0.661645, acc.: 54.69%] [G loss: 0.788884]\n",
      "807 [D loss: 0.662116, acc.: 59.38%] [G loss: 0.773335]\n",
      "808 [D loss: 0.607873, acc.: 68.75%] [G loss: 0.786098]\n",
      "809 [D loss: 0.621525, acc.: 71.88%] [G loss: 0.792703]\n",
      "810 [D loss: 0.615958, acc.: 64.06%] [G loss: 0.813210]\n",
      "811 [D loss: 0.658038, acc.: 64.06%] [G loss: 0.814423]\n",
      "812 [D loss: 0.649808, acc.: 62.50%] [G loss: 0.800910]\n",
      "813 [D loss: 0.617826, acc.: 67.19%] [G loss: 0.810481]\n",
      "814 [D loss: 0.653918, acc.: 56.25%] [G loss: 0.818641]\n",
      "815 [D loss: 0.600676, acc.: 73.44%] [G loss: 0.839480]\n",
      "816 [D loss: 0.657532, acc.: 62.50%] [G loss: 0.778426]\n",
      "817 [D loss: 0.593494, acc.: 70.31%] [G loss: 0.819602]\n",
      "818 [D loss: 0.592630, acc.: 73.44%] [G loss: 0.791624]\n",
      "819 [D loss: 0.596124, acc.: 62.50%] [G loss: 0.839171]\n",
      "820 [D loss: 0.606060, acc.: 71.88%] [G loss: 0.847324]\n",
      "821 [D loss: 0.639144, acc.: 62.50%] [G loss: 0.862522]\n",
      "822 [D loss: 0.653225, acc.: 59.38%] [G loss: 0.815980]\n",
      "823 [D loss: 0.652986, acc.: 62.50%] [G loss: 0.757776]\n",
      "824 [D loss: 0.642744, acc.: 71.88%] [G loss: 0.777845]\n",
      "825 [D loss: 0.630773, acc.: 70.31%] [G loss: 0.823688]\n",
      "826 [D loss: 0.630894, acc.: 68.75%] [G loss: 0.808306]\n",
      "827 [D loss: 0.619040, acc.: 67.19%] [G loss: 0.820421]\n",
      "828 [D loss: 0.612357, acc.: 65.62%] [G loss: 0.825565]\n",
      "829 [D loss: 0.603896, acc.: 73.44%] [G loss: 0.787960]\n",
      "830 [D loss: 0.636324, acc.: 65.62%] [G loss: 0.769179]\n",
      "831 [D loss: 0.607258, acc.: 71.88%] [G loss: 0.813495]\n",
      "832 [D loss: 0.604931, acc.: 68.75%] [G loss: 0.813972]\n",
      "833 [D loss: 0.643158, acc.: 60.94%] [G loss: 0.803333]\n",
      "834 [D loss: 0.619758, acc.: 65.62%] [G loss: 0.799403]\n",
      "835 [D loss: 0.677937, acc.: 51.56%] [G loss: 0.787485]\n",
      "836 [D loss: 0.644847, acc.: 57.81%] [G loss: 0.755708]\n",
      "837 [D loss: 0.681153, acc.: 56.25%] [G loss: 0.773791]\n",
      "838 [D loss: 0.640520, acc.: 60.94%] [G loss: 0.796508]\n",
      "839 [D loss: 0.629778, acc.: 64.06%] [G loss: 0.812586]\n",
      "840 [D loss: 0.604213, acc.: 70.31%] [G loss: 0.883395]\n",
      "841 [D loss: 0.624457, acc.: 71.88%] [G loss: 0.904543]\n",
      "842 [D loss: 0.624496, acc.: 71.88%] [G loss: 0.860289]\n",
      "843 [D loss: 0.620908, acc.: 73.44%] [G loss: 0.789552]\n",
      "844 [D loss: 0.643012, acc.: 60.94%] [G loss: 0.762398]\n",
      "845 [D loss: 0.604697, acc.: 70.31%] [G loss: 0.803409]\n",
      "846 [D loss: 0.595550, acc.: 76.56%] [G loss: 0.862644]\n",
      "847 [D loss: 0.643621, acc.: 62.50%] [G loss: 0.822050]\n",
      "848 [D loss: 0.640590, acc.: 64.06%] [G loss: 0.847048]\n",
      "849 [D loss: 0.647756, acc.: 51.56%] [G loss: 0.834257]\n",
      "850 [D loss: 0.615720, acc.: 65.62%] [G loss: 0.822504]\n",
      "851 [D loss: 0.630599, acc.: 60.94%] [G loss: 0.819585]\n",
      "852 [D loss: 0.670965, acc.: 59.38%] [G loss: 0.802204]\n",
      "853 [D loss: 0.630300, acc.: 67.19%] [G loss: 0.777747]\n",
      "854 [D loss: 0.644969, acc.: 60.94%] [G loss: 0.767504]\n",
      "855 [D loss: 0.649498, acc.: 57.81%] [G loss: 0.820113]\n",
      "856 [D loss: 0.641210, acc.: 59.38%] [G loss: 0.788618]\n",
      "857 [D loss: 0.624741, acc.: 68.75%] [G loss: 0.792518]\n",
      "858 [D loss: 0.652731, acc.: 62.50%] [G loss: 0.806632]\n",
      "859 [D loss: 0.639727, acc.: 64.06%] [G loss: 0.803979]\n",
      "860 [D loss: 0.609847, acc.: 75.00%] [G loss: 0.811245]\n",
      "861 [D loss: 0.631091, acc.: 68.75%] [G loss: 0.806515]\n",
      "862 [D loss: 0.600818, acc.: 71.88%] [G loss: 0.857825]\n",
      "863 [D loss: 0.628545, acc.: 68.75%] [G loss: 0.808944]\n",
      "864 [D loss: 0.596264, acc.: 78.12%] [G loss: 0.820346]\n",
      "865 [D loss: 0.617085, acc.: 70.31%] [G loss: 0.813659]\n",
      "866 [D loss: 0.622795, acc.: 68.75%] [G loss: 0.799277]\n",
      "867 [D loss: 0.614693, acc.: 62.50%] [G loss: 0.772616]\n",
      "868 [D loss: 0.609085, acc.: 71.88%] [G loss: 0.802248]\n",
      "869 [D loss: 0.622324, acc.: 73.44%] [G loss: 0.801223]\n",
      "870 [D loss: 0.620222, acc.: 62.50%] [G loss: 0.822536]\n",
      "871 [D loss: 0.609122, acc.: 67.19%] [G loss: 0.781693]\n",
      "872 [D loss: 0.611164, acc.: 70.31%] [G loss: 0.753192]\n",
      "873 [D loss: 0.603368, acc.: 62.50%] [G loss: 0.762150]\n",
      "874 [D loss: 0.601608, acc.: 70.31%] [G loss: 0.805068]\n",
      "875 [D loss: 0.637603, acc.: 60.94%] [G loss: 0.811617]\n",
      "876 [D loss: 0.629179, acc.: 67.19%] [G loss: 0.793749]\n",
      "877 [D loss: 0.629368, acc.: 68.75%] [G loss: 0.806779]\n",
      "878 [D loss: 0.676376, acc.: 51.56%] [G loss: 0.769561]\n",
      "879 [D loss: 0.623893, acc.: 62.50%] [G loss: 0.788531]\n",
      "880 [D loss: 0.630922, acc.: 65.62%] [G loss: 0.775537]\n",
      "881 [D loss: 0.629052, acc.: 62.50%] [G loss: 0.819150]\n",
      "882 [D loss: 0.635033, acc.: 67.19%] [G loss: 0.803350]\n",
      "883 [D loss: 0.692036, acc.: 48.44%] [G loss: 0.787686]\n",
      "884 [D loss: 0.631890, acc.: 68.75%] [G loss: 0.802845]\n",
      "885 [D loss: 0.618713, acc.: 60.94%] [G loss: 0.796327]\n",
      "886 [D loss: 0.640927, acc.: 65.62%] [G loss: 0.791737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887 [D loss: 0.600051, acc.: 70.31%] [G loss: 0.787597]\n",
      "888 [D loss: 0.607244, acc.: 67.19%] [G loss: 0.793436]\n",
      "889 [D loss: 0.627368, acc.: 59.38%] [G loss: 0.818939]\n",
      "890 [D loss: 0.608815, acc.: 70.31%] [G loss: 0.793801]\n",
      "891 [D loss: 0.626025, acc.: 71.88%] [G loss: 0.802948]\n",
      "892 [D loss: 0.637582, acc.: 64.06%] [G loss: 0.782724]\n",
      "893 [D loss: 0.614354, acc.: 64.06%] [G loss: 0.815331]\n",
      "894 [D loss: 0.606608, acc.: 64.06%] [G loss: 0.842756]\n",
      "895 [D loss: 0.599406, acc.: 73.44%] [G loss: 0.804134]\n",
      "896 [D loss: 0.655301, acc.: 57.81%] [G loss: 0.789476]\n",
      "897 [D loss: 0.605803, acc.: 70.31%] [G loss: 0.787119]\n",
      "898 [D loss: 0.598228, acc.: 67.19%] [G loss: 0.803959]\n",
      "899 [D loss: 0.606782, acc.: 70.31%] [G loss: 0.825329]\n",
      "900 [D loss: 0.635665, acc.: 62.50%] [G loss: 0.821648]\n",
      "901 [D loss: 0.649948, acc.: 57.81%] [G loss: 0.847337]\n",
      "902 [D loss: 0.598033, acc.: 73.44%] [G loss: 0.872109]\n",
      "903 [D loss: 0.615018, acc.: 67.19%] [G loss: 0.885124]\n",
      "904 [D loss: 0.625888, acc.: 73.44%] [G loss: 0.855983]\n",
      "905 [D loss: 0.609651, acc.: 68.75%] [G loss: 0.833152]\n",
      "906 [D loss: 0.672976, acc.: 56.25%] [G loss: 0.805132]\n",
      "907 [D loss: 0.632000, acc.: 57.81%] [G loss: 0.808870]\n",
      "908 [D loss: 0.601750, acc.: 76.56%] [G loss: 0.842793]\n",
      "909 [D loss: 0.614825, acc.: 67.19%] [G loss: 0.869247]\n",
      "910 [D loss: 0.606472, acc.: 71.88%] [G loss: 0.852476]\n",
      "911 [D loss: 0.637347, acc.: 64.06%] [G loss: 0.819551]\n",
      "912 [D loss: 0.604281, acc.: 71.88%] [G loss: 0.823254]\n",
      "913 [D loss: 0.626309, acc.: 62.50%] [G loss: 0.829738]\n",
      "914 [D loss: 0.588426, acc.: 79.69%] [G loss: 0.838025]\n",
      "915 [D loss: 0.616216, acc.: 67.19%] [G loss: 0.815666]\n",
      "916 [D loss: 0.595201, acc.: 76.56%] [G loss: 0.821957]\n",
      "917 [D loss: 0.587408, acc.: 71.88%] [G loss: 0.822131]\n",
      "918 [D loss: 0.571723, acc.: 78.12%] [G loss: 0.831026]\n",
      "919 [D loss: 0.636784, acc.: 59.38%] [G loss: 0.818455]\n",
      "920 [D loss: 0.636589, acc.: 57.81%] [G loss: 0.849128]\n",
      "921 [D loss: 0.584543, acc.: 75.00%] [G loss: 0.845174]\n",
      "922 [D loss: 0.605230, acc.: 71.88%] [G loss: 0.811230]\n",
      "923 [D loss: 0.632813, acc.: 64.06%] [G loss: 0.826993]\n",
      "924 [D loss: 0.619861, acc.: 64.06%] [G loss: 0.835652]\n",
      "925 [D loss: 0.589782, acc.: 75.00%] [G loss: 0.812113]\n",
      "926 [D loss: 0.568550, acc.: 81.25%] [G loss: 0.823829]\n",
      "927 [D loss: 0.618981, acc.: 64.06%] [G loss: 0.814599]\n",
      "928 [D loss: 0.603851, acc.: 67.19%] [G loss: 0.871738]\n",
      "929 [D loss: 0.625245, acc.: 68.75%] [G loss: 0.873563]\n",
      "930 [D loss: 0.642570, acc.: 62.50%] [G loss: 0.847676]\n",
      "931 [D loss: 0.635387, acc.: 64.06%] [G loss: 0.858279]\n",
      "932 [D loss: 0.628908, acc.: 67.19%] [G loss: 0.821076]\n",
      "933 [D loss: 0.592765, acc.: 73.44%] [G loss: 0.837398]\n",
      "934 [D loss: 0.595950, acc.: 68.75%] [G loss: 0.836998]\n",
      "935 [D loss: 0.623339, acc.: 57.81%] [G loss: 0.863481]\n",
      "936 [D loss: 0.628597, acc.: 65.62%] [G loss: 0.812160]\n",
      "937 [D loss: 0.665979, acc.: 54.69%] [G loss: 0.772532]\n",
      "938 [D loss: 0.645993, acc.: 54.69%] [G loss: 0.748214]\n",
      "939 [D loss: 0.615666, acc.: 64.06%] [G loss: 0.778959]\n",
      "940 [D loss: 0.631980, acc.: 60.94%] [G loss: 0.799914]\n",
      "941 [D loss: 0.618395, acc.: 65.62%] [G loss: 0.783638]\n",
      "942 [D loss: 0.616787, acc.: 79.69%] [G loss: 0.815431]\n",
      "943 [D loss: 0.611711, acc.: 70.31%] [G loss: 0.819678]\n",
      "944 [D loss: 0.645852, acc.: 60.94%] [G loss: 0.789177]\n",
      "945 [D loss: 0.621392, acc.: 59.38%] [G loss: 0.813920]\n",
      "946 [D loss: 0.607522, acc.: 62.50%] [G loss: 0.802623]\n",
      "947 [D loss: 0.616105, acc.: 68.75%] [G loss: 0.888628]\n",
      "948 [D loss: 0.606868, acc.: 75.00%] [G loss: 0.876707]\n",
      "949 [D loss: 0.634318, acc.: 68.75%] [G loss: 0.865446]\n",
      "950 [D loss: 0.644878, acc.: 57.81%] [G loss: 0.810254]\n",
      "951 [D loss: 0.637173, acc.: 62.50%] [G loss: 0.797413]\n",
      "952 [D loss: 0.621581, acc.: 68.75%] [G loss: 0.876369]\n",
      "953 [D loss: 0.613347, acc.: 71.88%] [G loss: 0.877430]\n",
      "954 [D loss: 0.565536, acc.: 89.06%] [G loss: 0.934928]\n",
      "955 [D loss: 0.657096, acc.: 60.94%] [G loss: 0.848540]\n",
      "956 [D loss: 0.622411, acc.: 67.19%] [G loss: 0.832377]\n",
      "957 [D loss: 0.582637, acc.: 82.81%] [G loss: 0.848473]\n",
      "958 [D loss: 0.609706, acc.: 64.06%] [G loss: 0.861079]\n",
      "959 [D loss: 0.618855, acc.: 67.19%] [G loss: 0.866931]\n",
      "960 [D loss: 0.575978, acc.: 76.56%] [G loss: 0.836885]\n",
      "961 [D loss: 0.630050, acc.: 64.06%] [G loss: 0.763991]\n",
      "962 [D loss: 0.618924, acc.: 60.94%] [G loss: 0.819981]\n",
      "963 [D loss: 0.606169, acc.: 71.88%] [G loss: 0.849582]\n",
      "964 [D loss: 0.623637, acc.: 70.31%] [G loss: 0.863113]\n",
      "965 [D loss: 0.580856, acc.: 81.25%] [G loss: 0.865776]\n",
      "966 [D loss: 0.599398, acc.: 75.00%] [G loss: 0.874809]\n",
      "967 [D loss: 0.607065, acc.: 62.50%] [G loss: 0.858869]\n",
      "968 [D loss: 0.597177, acc.: 75.00%] [G loss: 0.910763]\n",
      "969 [D loss: 0.612818, acc.: 75.00%] [G loss: 0.870034]\n",
      "970 [D loss: 0.630726, acc.: 67.19%] [G loss: 0.844477]\n",
      "971 [D loss: 0.573220, acc.: 78.12%] [G loss: 0.886822]\n",
      "972 [D loss: 0.579590, acc.: 76.56%] [G loss: 0.902104]\n",
      "973 [D loss: 0.580296, acc.: 78.12%] [G loss: 0.878414]\n",
      "974 [D loss: 0.585926, acc.: 76.56%] [G loss: 0.896957]\n",
      "975 [D loss: 0.644900, acc.: 60.94%] [G loss: 0.859370]\n",
      "976 [D loss: 0.593195, acc.: 70.31%] [G loss: 0.893075]\n",
      "977 [D loss: 0.627611, acc.: 68.75%] [G loss: 0.865454]\n",
      "978 [D loss: 0.587425, acc.: 75.00%] [G loss: 0.850284]\n",
      "979 [D loss: 0.579740, acc.: 76.56%] [G loss: 0.856223]\n",
      "980 [D loss: 0.588544, acc.: 76.56%] [G loss: 0.792172]\n",
      "981 [D loss: 0.600777, acc.: 68.75%] [G loss: 0.822979]\n",
      "982 [D loss: 0.634364, acc.: 62.50%] [G loss: 0.801701]\n",
      "983 [D loss: 0.606523, acc.: 67.19%] [G loss: 0.810550]\n",
      "984 [D loss: 0.629550, acc.: 65.62%] [G loss: 0.792161]\n",
      "985 [D loss: 0.618426, acc.: 67.19%] [G loss: 0.818754]\n",
      "986 [D loss: 0.606860, acc.: 67.19%] [G loss: 0.820343]\n",
      "987 [D loss: 0.651297, acc.: 59.38%] [G loss: 0.822464]\n",
      "988 [D loss: 0.662598, acc.: 50.00%] [G loss: 0.789798]\n",
      "989 [D loss: 0.614571, acc.: 65.62%] [G loss: 0.836261]\n",
      "990 [D loss: 0.625934, acc.: 68.75%] [G loss: 0.871649]\n",
      "991 [D loss: 0.595655, acc.: 76.56%] [G loss: 0.859899]\n",
      "992 [D loss: 0.592623, acc.: 78.12%] [G loss: 0.859563]\n",
      "993 [D loss: 0.607042, acc.: 75.00%] [G loss: 0.839781]\n",
      "994 [D loss: 0.609297, acc.: 68.75%] [G loss: 0.808046]\n",
      "995 [D loss: 0.629093, acc.: 64.06%] [G loss: 0.817539]\n",
      "996 [D loss: 0.599792, acc.: 65.62%] [G loss: 0.811346]\n",
      "997 [D loss: 0.559939, acc.: 82.81%] [G loss: 0.838292]\n",
      "998 [D loss: 0.603889, acc.: 68.75%] [G loss: 0.829973]\n",
      "999 [D loss: 0.589026, acc.: 76.56%] [G loss: 0.841936]\n",
      "1000 [D loss: 0.595720, acc.: 75.00%] [G loss: 0.885054]\n",
      "1001 [D loss: 0.607793, acc.: 78.12%] [G loss: 0.887882]\n",
      "1002 [D loss: 0.600358, acc.: 78.12%] [G loss: 0.878239]\n",
      "1003 [D loss: 0.619712, acc.: 71.88%] [G loss: 0.855297]\n",
      "1004 [D loss: 0.571517, acc.: 73.44%] [G loss: 0.870666]\n",
      "1005 [D loss: 0.589355, acc.: 76.56%] [G loss: 0.884717]\n",
      "1006 [D loss: 0.598619, acc.: 71.88%] [G loss: 0.893506]\n",
      "1007 [D loss: 0.623949, acc.: 62.50%] [G loss: 0.890362]\n",
      "1008 [D loss: 0.603679, acc.: 79.69%] [G loss: 0.871180]\n",
      "1009 [D loss: 0.613352, acc.: 68.75%] [G loss: 0.841749]\n",
      "1010 [D loss: 0.628912, acc.: 65.62%] [G loss: 0.832087]\n",
      "1011 [D loss: 0.555715, acc.: 82.81%] [G loss: 0.895393]\n",
      "1012 [D loss: 0.600482, acc.: 65.62%] [G loss: 0.870597]\n",
      "1013 [D loss: 0.571024, acc.: 81.25%] [G loss: 0.936986]\n",
      "1014 [D loss: 0.578422, acc.: 68.75%] [G loss: 0.901241]\n",
      "1015 [D loss: 0.606056, acc.: 68.75%] [G loss: 0.837992]\n",
      "1016 [D loss: 0.549126, acc.: 76.56%] [G loss: 0.899035]\n",
      "1017 [D loss: 0.613564, acc.: 68.75%] [G loss: 0.887628]\n",
      "1018 [D loss: 0.641399, acc.: 56.25%] [G loss: 0.888803]\n",
      "1019 [D loss: 0.600102, acc.: 71.88%] [G loss: 0.865990]\n",
      "1020 [D loss: 0.600700, acc.: 70.31%] [G loss: 0.855640]\n",
      "1021 [D loss: 0.605797, acc.: 73.44%] [G loss: 0.880287]\n",
      "1022 [D loss: 0.623830, acc.: 67.19%] [G loss: 0.872901]\n",
      "1023 [D loss: 0.592127, acc.: 65.62%] [G loss: 0.894336]\n",
      "1024 [D loss: 0.560920, acc.: 79.69%] [G loss: 0.880416]\n",
      "1025 [D loss: 0.608109, acc.: 76.56%] [G loss: 0.890208]\n",
      "1026 [D loss: 0.561144, acc.: 70.31%] [G loss: 0.850587]\n",
      "1027 [D loss: 0.580917, acc.: 76.56%] [G loss: 0.866961]\n",
      "1028 [D loss: 0.614305, acc.: 65.62%] [G loss: 0.869936]\n",
      "1029 [D loss: 0.590420, acc.: 75.00%] [G loss: 0.874541]\n",
      "1030 [D loss: 0.616459, acc.: 68.75%] [G loss: 0.867659]\n",
      "1031 [D loss: 0.609819, acc.: 70.31%] [G loss: 0.809224]\n",
      "1032 [D loss: 0.639021, acc.: 59.38%] [G loss: 0.884207]\n",
      "1033 [D loss: 0.664724, acc.: 57.81%] [G loss: 0.890529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1034 [D loss: 0.537844, acc.: 85.94%] [G loss: 0.926093]\n",
      "1035 [D loss: 0.566874, acc.: 82.81%] [G loss: 0.936471]\n",
      "1036 [D loss: 0.600901, acc.: 73.44%] [G loss: 0.873445]\n",
      "1037 [D loss: 0.576367, acc.: 79.69%] [G loss: 0.908608]\n",
      "1038 [D loss: 0.633784, acc.: 70.31%] [G loss: 0.875882]\n",
      "1039 [D loss: 0.604270, acc.: 71.88%] [G loss: 0.917000]\n",
      "1040 [D loss: 0.611861, acc.: 73.44%] [G loss: 0.877870]\n",
      "1041 [D loss: 0.590397, acc.: 73.44%] [G loss: 0.870021]\n",
      "1042 [D loss: 0.575261, acc.: 75.00%] [G loss: 0.847110]\n",
      "1043 [D loss: 0.595783, acc.: 79.69%] [G loss: 0.869876]\n",
      "1044 [D loss: 0.581152, acc.: 73.44%] [G loss: 0.879406]\n",
      "1045 [D loss: 0.605585, acc.: 64.06%] [G loss: 0.845664]\n",
      "1046 [D loss: 0.595882, acc.: 78.12%] [G loss: 0.876356]\n",
      "1047 [D loss: 0.595475, acc.: 71.88%] [G loss: 0.882981]\n",
      "1048 [D loss: 0.561725, acc.: 78.12%] [G loss: 0.900892]\n",
      "1049 [D loss: 0.616193, acc.: 71.88%] [G loss: 0.862519]\n",
      "1050 [D loss: 0.573112, acc.: 82.81%] [G loss: 0.900053]\n",
      "1051 [D loss: 0.585173, acc.: 71.88%] [G loss: 0.878369]\n",
      "1052 [D loss: 0.634146, acc.: 64.06%] [G loss: 0.853005]\n",
      "1053 [D loss: 0.636835, acc.: 62.50%] [G loss: 0.878315]\n",
      "1054 [D loss: 0.617406, acc.: 64.06%] [G loss: 0.877931]\n",
      "1055 [D loss: 0.609513, acc.: 73.44%] [G loss: 0.871936]\n",
      "1056 [D loss: 0.617321, acc.: 68.75%] [G loss: 0.840598]\n",
      "1057 [D loss: 0.609579, acc.: 70.31%] [G loss: 0.879878]\n",
      "1058 [D loss: 0.579580, acc.: 78.12%] [G loss: 0.864633]\n",
      "1059 [D loss: 0.559027, acc.: 70.31%] [G loss: 0.927417]\n",
      "1060 [D loss: 0.575365, acc.: 75.00%] [G loss: 0.936302]\n",
      "1061 [D loss: 0.625864, acc.: 65.62%] [G loss: 0.914212]\n",
      "1062 [D loss: 0.629430, acc.: 60.94%] [G loss: 0.870169]\n",
      "1063 [D loss: 0.599118, acc.: 76.56%] [G loss: 0.828613]\n",
      "1064 [D loss: 0.583173, acc.: 67.19%] [G loss: 0.881615]\n",
      "1065 [D loss: 0.620053, acc.: 62.50%] [G loss: 0.805073]\n",
      "1066 [D loss: 0.580483, acc.: 79.69%] [G loss: 0.864659]\n",
      "1067 [D loss: 0.649955, acc.: 64.06%] [G loss: 0.859366]\n",
      "1068 [D loss: 0.615007, acc.: 57.81%] [G loss: 0.816598]\n",
      "1069 [D loss: 0.649824, acc.: 62.50%] [G loss: 0.869457]\n",
      "1070 [D loss: 0.610700, acc.: 65.62%] [G loss: 0.887024]\n",
      "1071 [D loss: 0.617443, acc.: 71.88%] [G loss: 0.934774]\n",
      "1072 [D loss: 0.609684, acc.: 65.62%] [G loss: 0.961436]\n",
      "1073 [D loss: 0.599204, acc.: 73.44%] [G loss: 0.954930]\n",
      "1074 [D loss: 0.600285, acc.: 76.56%] [G loss: 0.915373]\n",
      "1075 [D loss: 0.635468, acc.: 62.50%] [G loss: 0.837175]\n",
      "1076 [D loss: 0.633931, acc.: 60.94%] [G loss: 0.854458]\n",
      "1077 [D loss: 0.595648, acc.: 70.31%] [G loss: 0.861338]\n",
      "1078 [D loss: 0.599374, acc.: 73.44%] [G loss: 0.880947]\n",
      "1079 [D loss: 0.603170, acc.: 65.62%] [G loss: 0.939571]\n",
      "1080 [D loss: 0.606057, acc.: 75.00%] [G loss: 0.920799]\n",
      "1081 [D loss: 0.586150, acc.: 68.75%] [G loss: 0.906987]\n",
      "1082 [D loss: 0.591728, acc.: 65.62%] [G loss: 0.986825]\n",
      "1083 [D loss: 0.623387, acc.: 60.94%] [G loss: 0.945708]\n",
      "1084 [D loss: 0.617626, acc.: 68.75%] [G loss: 0.844544]\n",
      "1085 [D loss: 0.599486, acc.: 65.62%] [G loss: 0.858127]\n",
      "1086 [D loss: 0.564046, acc.: 73.44%] [G loss: 0.895086]\n",
      "1087 [D loss: 0.535781, acc.: 82.81%] [G loss: 0.927909]\n",
      "1088 [D loss: 0.606169, acc.: 73.44%] [G loss: 0.985974]\n",
      "1089 [D loss: 0.579293, acc.: 79.69%] [G loss: 0.984736]\n",
      "1090 [D loss: 0.624317, acc.: 65.62%] [G loss: 0.893421]\n",
      "1091 [D loss: 0.554546, acc.: 79.69%] [G loss: 0.875704]\n",
      "1092 [D loss: 0.571328, acc.: 75.00%] [G loss: 0.917103]\n",
      "1093 [D loss: 0.610875, acc.: 68.75%] [G loss: 0.900786]\n",
      "1094 [D loss: 0.602881, acc.: 60.94%] [G loss: 0.934310]\n",
      "1095 [D loss: 0.632293, acc.: 67.19%] [G loss: 0.919080]\n",
      "1096 [D loss: 0.587422, acc.: 75.00%] [G loss: 0.838104]\n",
      "1097 [D loss: 0.634319, acc.: 56.25%] [G loss: 0.894503]\n",
      "1098 [D loss: 0.646126, acc.: 56.25%] [G loss: 0.926885]\n",
      "1099 [D loss: 0.567240, acc.: 82.81%] [G loss: 0.965194]\n",
      "1100 [D loss: 0.583248, acc.: 81.25%] [G loss: 0.897943]\n",
      "1101 [D loss: 0.617779, acc.: 67.19%] [G loss: 0.878211]\n",
      "1102 [D loss: 0.650111, acc.: 60.94%] [G loss: 0.890144]\n",
      "1103 [D loss: 0.586375, acc.: 71.88%] [G loss: 0.979411]\n",
      "1104 [D loss: 0.654778, acc.: 53.12%] [G loss: 0.897663]\n",
      "1105 [D loss: 0.567392, acc.: 81.25%] [G loss: 0.879100]\n",
      "1106 [D loss: 0.560341, acc.: 81.25%] [G loss: 0.902323]\n",
      "1107 [D loss: 0.577744, acc.: 71.88%] [G loss: 0.922830]\n",
      "1108 [D loss: 0.588619, acc.: 70.31%] [G loss: 0.901625]\n",
      "1109 [D loss: 0.548842, acc.: 84.38%] [G loss: 0.908472]\n",
      "1110 [D loss: 0.635166, acc.: 62.50%] [G loss: 0.866273]\n",
      "1111 [D loss: 0.569293, acc.: 82.81%] [G loss: 0.911750]\n",
      "1112 [D loss: 0.626786, acc.: 67.19%] [G loss: 0.859870]\n",
      "1113 [D loss: 0.581222, acc.: 82.81%] [G loss: 0.879155]\n",
      "1114 [D loss: 0.557798, acc.: 84.38%] [G loss: 0.898337]\n",
      "1115 [D loss: 0.565509, acc.: 75.00%] [G loss: 0.897715]\n",
      "1116 [D loss: 0.582878, acc.: 71.88%] [G loss: 0.872622]\n",
      "1117 [D loss: 0.623556, acc.: 60.94%] [G loss: 0.876910]\n",
      "1118 [D loss: 0.656909, acc.: 60.94%] [G loss: 0.872571]\n",
      "1119 [D loss: 0.574813, acc.: 70.31%] [G loss: 0.893067]\n",
      "1120 [D loss: 0.598612, acc.: 71.88%] [G loss: 0.904179]\n",
      "1121 [D loss: 0.577682, acc.: 78.12%] [G loss: 0.933839]\n",
      "1122 [D loss: 0.570086, acc.: 78.12%] [G loss: 0.931509]\n",
      "1123 [D loss: 0.604940, acc.: 67.19%] [G loss: 0.905925]\n",
      "1124 [D loss: 0.608015, acc.: 68.75%] [G loss: 0.900906]\n",
      "1125 [D loss: 0.605080, acc.: 59.38%] [G loss: 0.897806]\n",
      "1126 [D loss: 0.602343, acc.: 68.75%] [G loss: 0.864128]\n",
      "1127 [D loss: 0.614564, acc.: 67.19%] [G loss: 0.889445]\n",
      "1128 [D loss: 0.585291, acc.: 76.56%] [G loss: 0.916270]\n",
      "1129 [D loss: 0.618832, acc.: 73.44%] [G loss: 0.894238]\n",
      "1130 [D loss: 0.597688, acc.: 68.75%] [G loss: 0.898917]\n",
      "1131 [D loss: 0.584698, acc.: 75.00%] [G loss: 0.890749]\n",
      "1132 [D loss: 0.572455, acc.: 84.38%] [G loss: 0.887063]\n",
      "1133 [D loss: 0.593198, acc.: 62.50%] [G loss: 0.851829]\n",
      "1134 [D loss: 0.538189, acc.: 84.38%] [G loss: 0.876922]\n",
      "1135 [D loss: 0.613961, acc.: 73.44%] [G loss: 0.807045]\n",
      "1136 [D loss: 0.595699, acc.: 68.75%] [G loss: 0.847690]\n",
      "1137 [D loss: 0.565977, acc.: 82.81%] [G loss: 0.880049]\n",
      "1138 [D loss: 0.606354, acc.: 68.75%] [G loss: 0.883908]\n",
      "1139 [D loss: 0.583119, acc.: 76.56%] [G loss: 0.841404]\n",
      "1140 [D loss: 0.655586, acc.: 56.25%] [G loss: 0.847237]\n",
      "1141 [D loss: 0.593882, acc.: 70.31%] [G loss: 0.874225]\n",
      "1142 [D loss: 0.596506, acc.: 71.88%] [G loss: 0.900502]\n",
      "1143 [D loss: 0.590534, acc.: 68.75%] [G loss: 0.925378]\n",
      "1144 [D loss: 0.595452, acc.: 65.62%] [G loss: 0.930266]\n",
      "1145 [D loss: 0.579467, acc.: 78.12%] [G loss: 0.959203]\n",
      "1146 [D loss: 0.570282, acc.: 79.69%] [G loss: 0.841177]\n",
      "1147 [D loss: 0.629010, acc.: 68.75%] [G loss: 0.877781]\n",
      "1148 [D loss: 0.577503, acc.: 76.56%] [G loss: 0.867771]\n",
      "1149 [D loss: 0.608733, acc.: 71.88%] [G loss: 0.864508]\n",
      "1150 [D loss: 0.574154, acc.: 78.12%] [G loss: 0.904699]\n",
      "1151 [D loss: 0.575583, acc.: 73.44%] [G loss: 0.920849]\n",
      "1152 [D loss: 0.581136, acc.: 78.12%] [G loss: 0.945092]\n",
      "1153 [D loss: 0.610035, acc.: 70.31%] [G loss: 0.861701]\n",
      "1154 [D loss: 0.633252, acc.: 57.81%] [G loss: 0.902252]\n",
      "1155 [D loss: 0.600807, acc.: 68.75%] [G loss: 0.847331]\n",
      "1156 [D loss: 0.548650, acc.: 84.38%] [G loss: 0.901698]\n",
      "1157 [D loss: 0.578506, acc.: 67.19%] [G loss: 0.905042]\n",
      "1158 [D loss: 0.652030, acc.: 56.25%] [G loss: 0.923222]\n",
      "1159 [D loss: 0.591440, acc.: 73.44%] [G loss: 0.908232]\n",
      "1160 [D loss: 0.597549, acc.: 76.56%] [G loss: 0.870174]\n",
      "1161 [D loss: 0.587304, acc.: 76.56%] [G loss: 0.865949]\n",
      "1162 [D loss: 0.636092, acc.: 65.62%] [G loss: 0.893441]\n",
      "1163 [D loss: 0.623422, acc.: 65.62%] [G loss: 0.963150]\n",
      "1164 [D loss: 0.619071, acc.: 73.44%] [G loss: 0.900883]\n",
      "1165 [D loss: 0.584600, acc.: 73.44%] [G loss: 0.886657]\n",
      "1166 [D loss: 0.643823, acc.: 60.94%] [G loss: 0.904180]\n",
      "1167 [D loss: 0.661224, acc.: 56.25%] [G loss: 0.870255]\n",
      "1168 [D loss: 0.643734, acc.: 57.81%] [G loss: 0.920917]\n",
      "1169 [D loss: 0.576435, acc.: 82.81%] [G loss: 0.907446]\n",
      "1170 [D loss: 0.653493, acc.: 67.19%] [G loss: 0.912284]\n",
      "1171 [D loss: 0.654068, acc.: 53.12%] [G loss: 0.866165]\n",
      "1172 [D loss: 0.609344, acc.: 65.62%] [G loss: 0.919480]\n",
      "1173 [D loss: 0.564088, acc.: 79.69%] [G loss: 0.930196]\n",
      "1174 [D loss: 0.575204, acc.: 82.81%] [G loss: 0.955025]\n",
      "1175 [D loss: 0.532995, acc.: 84.38%] [G loss: 0.975243]\n",
      "1176 [D loss: 0.543822, acc.: 79.69%] [G loss: 0.935017]\n",
      "1177 [D loss: 0.603331, acc.: 68.75%] [G loss: 0.929342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1178 [D loss: 0.606752, acc.: 68.75%] [G loss: 0.924847]\n",
      "1179 [D loss: 0.625308, acc.: 62.50%] [G loss: 0.966372]\n",
      "1180 [D loss: 0.572664, acc.: 79.69%] [G loss: 0.923102]\n",
      "1181 [D loss: 0.607483, acc.: 70.31%] [G loss: 0.899372]\n",
      "1182 [D loss: 0.580021, acc.: 71.88%] [G loss: 0.877638]\n",
      "1183 [D loss: 0.560993, acc.: 73.44%] [G loss: 0.909315]\n",
      "1184 [D loss: 0.561012, acc.: 81.25%] [G loss: 0.881297]\n",
      "1185 [D loss: 0.604904, acc.: 67.19%] [G loss: 0.921420]\n",
      "1186 [D loss: 0.577526, acc.: 75.00%] [G loss: 0.928809]\n",
      "1187 [D loss: 0.594941, acc.: 75.00%] [G loss: 0.906286]\n",
      "1188 [D loss: 0.545067, acc.: 76.56%] [G loss: 0.968898]\n",
      "1189 [D loss: 0.695269, acc.: 48.44%] [G loss: 0.869926]\n",
      "1190 [D loss: 0.544547, acc.: 81.25%] [G loss: 0.993310]\n",
      "1191 [D loss: 0.526085, acc.: 87.50%] [G loss: 1.000656]\n",
      "1192 [D loss: 0.580419, acc.: 76.56%] [G loss: 0.929203]\n",
      "1193 [D loss: 0.552430, acc.: 82.81%] [G loss: 0.902621]\n",
      "1194 [D loss: 0.563710, acc.: 68.75%] [G loss: 0.903166]\n",
      "1195 [D loss: 0.618345, acc.: 68.75%] [G loss: 0.935306]\n",
      "1196 [D loss: 0.551439, acc.: 76.56%] [G loss: 0.941102]\n",
      "1197 [D loss: 0.567441, acc.: 70.31%] [G loss: 0.945572]\n",
      "1198 [D loss: 0.612698, acc.: 71.88%] [G loss: 0.886971]\n",
      "1199 [D loss: 0.535320, acc.: 76.56%] [G loss: 0.916273]\n",
      "1200 [D loss: 0.601371, acc.: 64.06%] [G loss: 0.888820]\n",
      "1201 [D loss: 0.573447, acc.: 73.44%] [G loss: 0.925227]\n",
      "1202 [D loss: 0.576629, acc.: 67.19%] [G loss: 0.987975]\n",
      "1203 [D loss: 0.581814, acc.: 75.00%] [G loss: 0.956585]\n",
      "1204 [D loss: 0.586062, acc.: 75.00%] [G loss: 0.923845]\n",
      "1205 [D loss: 0.629512, acc.: 67.19%] [G loss: 0.920248]\n",
      "1206 [D loss: 0.589201, acc.: 73.44%] [G loss: 0.945089]\n",
      "1207 [D loss: 0.571722, acc.: 70.31%] [G loss: 0.968073]\n",
      "1208 [D loss: 0.579130, acc.: 78.12%] [G loss: 0.979347]\n",
      "1209 [D loss: 0.583627, acc.: 76.56%] [G loss: 0.968408]\n",
      "1210 [D loss: 0.604962, acc.: 76.56%] [G loss: 0.908281]\n",
      "1211 [D loss: 0.581789, acc.: 75.00%] [G loss: 0.924194]\n",
      "1212 [D loss: 0.607162, acc.: 73.44%] [G loss: 0.974938]\n",
      "1213 [D loss: 0.569558, acc.: 76.56%] [G loss: 0.944859]\n",
      "1214 [D loss: 0.565036, acc.: 75.00%] [G loss: 0.953309]\n",
      "1215 [D loss: 0.619634, acc.: 65.62%] [G loss: 0.901129]\n",
      "1216 [D loss: 0.601399, acc.: 70.31%] [G loss: 0.890692]\n",
      "1217 [D loss: 0.628716, acc.: 64.06%] [G loss: 0.888270]\n",
      "1218 [D loss: 0.622073, acc.: 70.31%] [G loss: 0.893064]\n",
      "1219 [D loss: 0.561330, acc.: 81.25%] [G loss: 0.969047]\n",
      "1220 [D loss: 0.597583, acc.: 76.56%] [G loss: 1.037374]\n",
      "1221 [D loss: 0.613378, acc.: 70.31%] [G loss: 0.986709]\n",
      "1222 [D loss: 0.636031, acc.: 65.62%] [G loss: 0.919949]\n",
      "1223 [D loss: 0.610281, acc.: 73.44%] [G loss: 0.942786]\n",
      "1224 [D loss: 0.601824, acc.: 71.88%] [G loss: 0.933938]\n",
      "1225 [D loss: 0.624144, acc.: 68.75%] [G loss: 0.941973]\n",
      "1226 [D loss: 0.604073, acc.: 67.19%] [G loss: 0.893182]\n",
      "1227 [D loss: 0.541274, acc.: 84.38%] [G loss: 0.938247]\n",
      "1228 [D loss: 0.562417, acc.: 78.12%] [G loss: 0.942461]\n",
      "1229 [D loss: 0.550657, acc.: 79.69%] [G loss: 0.910138]\n",
      "1230 [D loss: 0.560697, acc.: 81.25%] [G loss: 0.927771]\n",
      "1231 [D loss: 0.560126, acc.: 76.56%] [G loss: 0.894066]\n",
      "1232 [D loss: 0.620340, acc.: 56.25%] [G loss: 0.976057]\n",
      "1233 [D loss: 0.554482, acc.: 79.69%] [G loss: 0.918194]\n",
      "1234 [D loss: 0.573687, acc.: 68.75%] [G loss: 0.913239]\n",
      "1235 [D loss: 0.550855, acc.: 78.12%] [G loss: 0.940208]\n",
      "1236 [D loss: 0.589508, acc.: 75.00%] [G loss: 0.925402]\n",
      "1237 [D loss: 0.650982, acc.: 59.38%] [G loss: 0.903182]\n",
      "1238 [D loss: 0.575895, acc.: 71.88%] [G loss: 0.919416]\n",
      "1239 [D loss: 0.593124, acc.: 75.00%] [G loss: 0.913205]\n",
      "1240 [D loss: 0.600451, acc.: 75.00%] [G loss: 0.936549]\n",
      "1241 [D loss: 0.604077, acc.: 75.00%] [G loss: 0.892720]\n",
      "1242 [D loss: 0.633459, acc.: 65.62%] [G loss: 0.893139]\n",
      "1243 [D loss: 0.590184, acc.: 70.31%] [G loss: 0.843505]\n",
      "1244 [D loss: 0.608584, acc.: 76.56%] [G loss: 0.907160]\n",
      "1245 [D loss: 0.563098, acc.: 84.38%] [G loss: 0.989947]\n",
      "1246 [D loss: 0.570662, acc.: 85.94%] [G loss: 0.943111]\n",
      "1247 [D loss: 0.549575, acc.: 79.69%] [G loss: 0.908025]\n",
      "1248 [D loss: 0.557716, acc.: 82.81%] [G loss: 0.950362]\n",
      "1249 [D loss: 0.548312, acc.: 79.69%] [G loss: 0.982799]\n",
      "1250 [D loss: 0.608527, acc.: 65.62%] [G loss: 1.000155]\n",
      "1251 [D loss: 0.563152, acc.: 73.44%] [G loss: 0.999764]\n",
      "1252 [D loss: 0.568369, acc.: 76.56%] [G loss: 0.942553]\n",
      "1253 [D loss: 0.530711, acc.: 79.69%] [G loss: 0.969047]\n",
      "1254 [D loss: 0.636432, acc.: 60.94%] [G loss: 0.893042]\n",
      "1255 [D loss: 0.569220, acc.: 73.44%] [G loss: 0.879182]\n",
      "1256 [D loss: 0.595947, acc.: 68.75%] [G loss: 0.874074]\n",
      "1257 [D loss: 0.596805, acc.: 71.88%] [G loss: 0.883617]\n",
      "1258 [D loss: 0.548585, acc.: 79.69%] [G loss: 0.919232]\n",
      "1259 [D loss: 0.591484, acc.: 73.44%] [G loss: 0.978040]\n",
      "1260 [D loss: 0.597964, acc.: 73.44%] [G loss: 0.964102]\n",
      "1261 [D loss: 0.604936, acc.: 70.31%] [G loss: 0.923627]\n",
      "1262 [D loss: 0.553566, acc.: 79.69%] [G loss: 0.989148]\n",
      "1263 [D loss: 0.588587, acc.: 71.88%] [G loss: 1.039452]\n",
      "1264 [D loss: 0.569338, acc.: 71.88%] [G loss: 1.018362]\n",
      "1265 [D loss: 0.616768, acc.: 73.44%] [G loss: 0.934634]\n",
      "1266 [D loss: 0.599108, acc.: 75.00%] [G loss: 0.918007]\n",
      "1267 [D loss: 0.532401, acc.: 84.38%] [G loss: 0.982827]\n",
      "1268 [D loss: 0.573557, acc.: 71.88%] [G loss: 0.993031]\n",
      "1269 [D loss: 0.664809, acc.: 67.19%] [G loss: 0.927267]\n",
      "1270 [D loss: 0.545549, acc.: 76.56%] [G loss: 0.926833]\n",
      "1271 [D loss: 0.605551, acc.: 64.06%] [G loss: 0.956494]\n",
      "1272 [D loss: 0.624196, acc.: 64.06%] [G loss: 0.969798]\n",
      "1273 [D loss: 0.613093, acc.: 71.88%] [G loss: 0.919738]\n",
      "1274 [D loss: 0.631761, acc.: 68.75%] [G loss: 0.905522]\n",
      "1275 [D loss: 0.583575, acc.: 76.56%] [G loss: 0.918673]\n",
      "1276 [D loss: 0.582060, acc.: 71.88%] [G loss: 0.924268]\n",
      "1277 [D loss: 0.625706, acc.: 62.50%] [G loss: 0.849113]\n",
      "1278 [D loss: 0.512861, acc.: 85.94%] [G loss: 0.844037]\n",
      "1279 [D loss: 0.577064, acc.: 73.44%] [G loss: 0.905193]\n",
      "1280 [D loss: 0.598740, acc.: 73.44%] [G loss: 0.857408]\n",
      "1281 [D loss: 0.607165, acc.: 73.44%] [G loss: 0.885184]\n",
      "1282 [D loss: 0.593802, acc.: 68.75%] [G loss: 0.841832]\n",
      "1283 [D loss: 0.600673, acc.: 68.75%] [G loss: 0.849232]\n",
      "1284 [D loss: 0.601029, acc.: 70.31%] [G loss: 0.912226]\n",
      "1285 [D loss: 0.611031, acc.: 68.75%] [G loss: 0.870124]\n",
      "1286 [D loss: 0.606025, acc.: 70.31%] [G loss: 0.924968]\n",
      "1287 [D loss: 0.615588, acc.: 67.19%] [G loss: 0.886077]\n",
      "1288 [D loss: 0.633830, acc.: 65.62%] [G loss: 0.887526]\n",
      "1289 [D loss: 0.608492, acc.: 67.19%] [G loss: 0.882293]\n",
      "1290 [D loss: 0.606136, acc.: 70.31%] [G loss: 0.926340]\n",
      "1291 [D loss: 0.604335, acc.: 70.31%] [G loss: 0.882817]\n",
      "1292 [D loss: 0.592512, acc.: 70.31%] [G loss: 0.880948]\n",
      "1293 [D loss: 0.627836, acc.: 68.75%] [G loss: 0.855955]\n",
      "1294 [D loss: 0.614603, acc.: 65.62%] [G loss: 0.846616]\n",
      "1295 [D loss: 0.601875, acc.: 64.06%] [G loss: 0.842078]\n",
      "1296 [D loss: 0.599548, acc.: 68.75%] [G loss: 0.920215]\n",
      "1297 [D loss: 0.635764, acc.: 64.06%] [G loss: 1.001125]\n",
      "1298 [D loss: 0.565510, acc.: 81.25%] [G loss: 0.941765]\n",
      "1299 [D loss: 0.603296, acc.: 71.88%] [G loss: 0.910668]\n",
      "1300 [D loss: 0.643267, acc.: 62.50%] [G loss: 0.860594]\n",
      "1301 [D loss: 0.565090, acc.: 79.69%] [G loss: 0.941709]\n",
      "1302 [D loss: 0.562901, acc.: 75.00%] [G loss: 0.901804]\n",
      "1303 [D loss: 0.569821, acc.: 71.88%] [G loss: 0.881843]\n",
      "1304 [D loss: 0.593997, acc.: 70.31%] [G loss: 0.938402]\n",
      "1305 [D loss: 0.581951, acc.: 70.31%] [G loss: 0.903274]\n",
      "1306 [D loss: 0.585442, acc.: 76.56%] [G loss: 0.969204]\n",
      "1307 [D loss: 0.538258, acc.: 81.25%] [G loss: 0.956873]\n",
      "1308 [D loss: 0.554941, acc.: 81.25%] [G loss: 0.962267]\n",
      "1309 [D loss: 0.530299, acc.: 84.38%] [G loss: 0.871590]\n",
      "1310 [D loss: 0.517044, acc.: 82.81%] [G loss: 0.931462]\n",
      "1311 [D loss: 0.550612, acc.: 79.69%] [G loss: 1.003059]\n",
      "1312 [D loss: 0.574360, acc.: 70.31%] [G loss: 0.922621]\n",
      "1313 [D loss: 0.548059, acc.: 78.12%] [G loss: 0.942692]\n",
      "1314 [D loss: 0.611026, acc.: 70.31%] [G loss: 0.851726]\n",
      "1315 [D loss: 0.586804, acc.: 73.44%] [G loss: 0.871154]\n",
      "1316 [D loss: 0.560581, acc.: 76.56%] [G loss: 0.950233]\n",
      "1317 [D loss: 0.574894, acc.: 71.88%] [G loss: 0.979531]\n",
      "1318 [D loss: 0.544974, acc.: 87.50%] [G loss: 0.991275]\n",
      "1319 [D loss: 0.621983, acc.: 67.19%] [G loss: 0.949593]\n",
      "1320 [D loss: 0.564251, acc.: 75.00%] [G loss: 0.883944]\n",
      "1321 [D loss: 0.541064, acc.: 76.56%] [G loss: 0.980108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1322 [D loss: 0.612187, acc.: 65.62%] [G loss: 0.907871]\n",
      "1323 [D loss: 0.602788, acc.: 68.75%] [G loss: 0.887438]\n",
      "1324 [D loss: 0.616627, acc.: 70.31%] [G loss: 0.858368]\n",
      "1325 [D loss: 0.571182, acc.: 70.31%] [G loss: 0.836175]\n",
      "1326 [D loss: 0.629228, acc.: 62.50%] [G loss: 0.954596]\n",
      "1327 [D loss: 0.583036, acc.: 75.00%] [G loss: 0.953842]\n",
      "1328 [D loss: 0.639980, acc.: 65.62%] [G loss: 0.919710]\n",
      "1329 [D loss: 0.642002, acc.: 59.38%] [G loss: 0.892883]\n",
      "1330 [D loss: 0.567804, acc.: 73.44%] [G loss: 0.943448]\n",
      "1331 [D loss: 0.646589, acc.: 60.94%] [G loss: 0.901417]\n",
      "1332 [D loss: 0.616387, acc.: 65.62%] [G loss: 0.968925]\n",
      "1333 [D loss: 0.540743, acc.: 81.25%] [G loss: 0.978859]\n",
      "1334 [D loss: 0.585187, acc.: 67.19%] [G loss: 1.005361]\n",
      "1335 [D loss: 0.608524, acc.: 76.56%] [G loss: 0.891405]\n",
      "1336 [D loss: 0.580431, acc.: 71.88%] [G loss: 0.915802]\n",
      "1337 [D loss: 0.611278, acc.: 64.06%] [G loss: 0.911230]\n",
      "1338 [D loss: 0.577546, acc.: 70.31%] [G loss: 0.950059]\n",
      "1339 [D loss: 0.618587, acc.: 65.62%] [G loss: 0.868274]\n",
      "1340 [D loss: 0.572631, acc.: 70.31%] [G loss: 0.914753]\n",
      "1341 [D loss: 0.586059, acc.: 75.00%] [G loss: 0.974874]\n",
      "1342 [D loss: 0.564040, acc.: 71.88%] [G loss: 0.954497]\n",
      "1343 [D loss: 0.633945, acc.: 62.50%] [G loss: 0.973794]\n",
      "1344 [D loss: 0.623599, acc.: 64.06%] [G loss: 0.921465]\n",
      "1345 [D loss: 0.613152, acc.: 68.75%] [G loss: 0.963505]\n",
      "1346 [D loss: 0.653184, acc.: 51.56%] [G loss: 0.889046]\n",
      "1347 [D loss: 0.544923, acc.: 76.56%] [G loss: 0.896449]\n",
      "1348 [D loss: 0.613433, acc.: 64.06%] [G loss: 0.933383]\n",
      "1349 [D loss: 0.613608, acc.: 68.75%] [G loss: 0.915273]\n",
      "1350 [D loss: 0.614355, acc.: 65.62%] [G loss: 0.935255]\n",
      "1351 [D loss: 0.584059, acc.: 73.44%] [G loss: 0.908656]\n",
      "1352 [D loss: 0.524158, acc.: 71.88%] [G loss: 0.964475]\n",
      "1353 [D loss: 0.583931, acc.: 73.44%] [G loss: 0.925191]\n",
      "1354 [D loss: 0.563413, acc.: 71.88%] [G loss: 0.906684]\n",
      "1355 [D loss: 0.610743, acc.: 65.62%] [G loss: 0.889179]\n",
      "1356 [D loss: 0.546776, acc.: 71.88%] [G loss: 0.903268]\n",
      "1357 [D loss: 0.556570, acc.: 76.56%] [G loss: 0.958974]\n",
      "1358 [D loss: 0.591918, acc.: 75.00%] [G loss: 0.911466]\n",
      "1359 [D loss: 0.590770, acc.: 68.75%] [G loss: 0.847090]\n",
      "1360 [D loss: 0.631209, acc.: 64.06%] [G loss: 0.900321]\n",
      "1361 [D loss: 0.614453, acc.: 62.50%] [G loss: 0.884223]\n",
      "1362 [D loss: 0.547930, acc.: 73.44%] [G loss: 0.946620]\n",
      "1363 [D loss: 0.654902, acc.: 56.25%] [G loss: 0.836844]\n",
      "1364 [D loss: 0.655442, acc.: 56.25%] [G loss: 0.901019]\n",
      "1365 [D loss: 0.659863, acc.: 62.50%] [G loss: 0.898696]\n",
      "1366 [D loss: 0.586585, acc.: 70.31%] [G loss: 0.873635]\n",
      "1367 [D loss: 0.626162, acc.: 68.75%] [G loss: 0.783805]\n",
      "1368 [D loss: 0.606334, acc.: 73.44%] [G loss: 0.862218]\n",
      "1369 [D loss: 0.670710, acc.: 54.69%] [G loss: 0.862620]\n",
      "1370 [D loss: 0.615790, acc.: 62.50%] [G loss: 0.898729]\n",
      "1371 [D loss: 0.667411, acc.: 59.38%] [G loss: 0.855048]\n",
      "1372 [D loss: 0.615825, acc.: 64.06%] [G loss: 0.948625]\n",
      "1373 [D loss: 0.631349, acc.: 59.38%] [G loss: 0.928499]\n",
      "1374 [D loss: 0.585105, acc.: 73.44%] [G loss: 0.933553]\n",
      "1375 [D loss: 0.595087, acc.: 70.31%] [G loss: 0.934616]\n",
      "1376 [D loss: 0.598708, acc.: 65.62%] [G loss: 0.963189]\n",
      "1377 [D loss: 0.610539, acc.: 73.44%] [G loss: 0.836893]\n",
      "1378 [D loss: 0.594613, acc.: 64.06%] [G loss: 0.878942]\n",
      "1379 [D loss: 0.550021, acc.: 82.81%] [G loss: 0.921438]\n",
      "1380 [D loss: 0.617448, acc.: 68.75%] [G loss: 0.880766]\n",
      "1381 [D loss: 0.565733, acc.: 76.56%] [G loss: 0.906974]\n",
      "1382 [D loss: 0.649199, acc.: 54.69%] [G loss: 0.846643]\n",
      "1383 [D loss: 0.587679, acc.: 73.44%] [G loss: 0.925719]\n",
      "1384 [D loss: 0.600252, acc.: 64.06%] [G loss: 0.956118]\n",
      "1385 [D loss: 0.648255, acc.: 67.19%] [G loss: 0.880769]\n",
      "1386 [D loss: 0.598090, acc.: 75.00%] [G loss: 0.881243]\n",
      "1387 [D loss: 0.601339, acc.: 67.19%] [G loss: 0.899649]\n",
      "1388 [D loss: 0.609381, acc.: 67.19%] [G loss: 0.913314]\n",
      "1389 [D loss: 0.631491, acc.: 70.31%] [G loss: 0.843986]\n",
      "1390 [D loss: 0.566731, acc.: 65.62%] [G loss: 0.876799]\n",
      "1391 [D loss: 0.570635, acc.: 75.00%] [G loss: 0.861144]\n",
      "1392 [D loss: 0.574150, acc.: 73.44%] [G loss: 0.900875]\n",
      "1393 [D loss: 0.549442, acc.: 79.69%] [G loss: 0.846773]\n",
      "1394 [D loss: 0.610029, acc.: 68.75%] [G loss: 0.859150]\n",
      "1395 [D loss: 0.586119, acc.: 71.88%] [G loss: 0.864032]\n",
      "1396 [D loss: 0.623978, acc.: 62.50%] [G loss: 0.861930]\n",
      "1397 [D loss: 0.568027, acc.: 79.69%] [G loss: 0.878032]\n",
      "1398 [D loss: 0.593706, acc.: 71.88%] [G loss: 0.945807]\n",
      "1399 [D loss: 0.565840, acc.: 78.12%] [G loss: 0.918638]\n",
      "1400 [D loss: 0.623721, acc.: 71.88%] [G loss: 0.879066]\n",
      "1401 [D loss: 0.557006, acc.: 75.00%] [G loss: 0.908526]\n",
      "1402 [D loss: 0.582932, acc.: 68.75%] [G loss: 0.968109]\n",
      "1403 [D loss: 0.631270, acc.: 68.75%] [G loss: 0.881531]\n",
      "1404 [D loss: 0.598742, acc.: 73.44%] [G loss: 0.930764]\n",
      "1405 [D loss: 0.625405, acc.: 71.88%] [G loss: 0.961653]\n",
      "1406 [D loss: 0.599055, acc.: 75.00%] [G loss: 0.932753]\n",
      "1407 [D loss: 0.631505, acc.: 60.94%] [G loss: 0.937421]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-efcfe2e6b612>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-33bfd3bc99e0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;31m# Train the generator (to have the discriminator label samples as valid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# Plot the progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(epochs=30000, batch_size=32, sample_interval=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
